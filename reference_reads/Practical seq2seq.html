<!DOCTYPE html>
<html lang="en"><!-- Beautiful Jekyll | MIT license | Copyright Dean Attali 2016 --><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Practical seq2seq</title>

  <meta name="author" content="Suriyadeepan Ramamoorthy">
  
  
  <meta name="description" content="Revisiting sequence to sequence learning, with focus on implementation details">
  

  <link rel="alternate" type="application/rss+xml" title="Suriyadeepan Ram - My personal blog!" href="http://suriyadeepan.github.io/feed.xml">
  
 <!-- everything has to be repeated twice because on 2016-02-01 GitHub pages migrated to jekyll 3; see bug https://github.com/jekyll/jekyll/issues/4439 -->

    
  
    
      <link rel="stylesheet" href="Practical%20seq2seq_files/font-awesome.css">
    
      
  
  
  
    
      <link rel="stylesheet" href="Practical%20seq2seq_files/bootstrap.css">
    
      <link rel="stylesheet" href="Practical%20seq2seq_files/main.css">
    
    
  
  
  
    
      <link rel="stylesheet" href="Practical%20seq2seq_files/css.css">
    
      <link rel="stylesheet" href="Practical%20seq2seq_files/css_002.css">
    
  

    
    
  
  
  
  
  
     
  
  <!-- Facebook OpenGraph tags -->
  <meta property="og:title" content="Practical seq2seq">
  <meta property="og:type" content="website">
  
  
  <meta property="og:url" content="http://suriyadeepan.github.io/2016-12-31-practical-seq2seq//">
  
  
  
  <meta property="og:image" content="http://suriyadeepan.github.io/img/avatar-icon.png">
  
  
<script async="" src="Practical%20seq2seq_files/analytics.js"></script><script type="text/javascript" async="" src="Practical%20seq2seq_files/embed.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><link rel="prefetch" as="style" href="Practical%20seq2seq_files/a_data/lounge.css"><link rel="prefetch" as="script" href="Practical%20seq2seq_files/a_data/common.js"><link rel="prefetch" as="script" href="Practical%20seq2seq_files/a_data/lounge.js"><link rel="prefetch" as="script" href="Practical%20seq2seq_files/a_data/config.js"><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><script src="Practical%20seq2seq_files/alfalfa.js" async="" charset="UTF-8"></script></head>


  <body class="vsc-initialized"><div id="MathJax_Message" style="display: none;"></div>
  
    
<nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://suriyadeepan.github.io/">Suriyadeepan Ram</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
          <li>
            
            





<a href="http://suriyadeepan.github.io/aboutme">About Me</a>

          </li>
        
        
        
          <li class="navlinks-container" style="min-width: 115px;">
            <a class="navlinks-parent" href="javascript:void(0)">Slides</a>
            <div class="navlinks-children">
              
                
                  
            





<a href="http://pymeshnet.gitlab.io/slides/bangpypermeet/software/">Mesh Network</a>

                
              
                
                  
            





<a href="https://suriyadeepan.github.io/python3">Python 3</a>

                
              
                
                  
            





<a href="https://suriyadeepan.github.io/4ccon">Machine Learning (4ccon)</a>

                
              
                
                  
            





<a href="http://slides.com/suriyadeepanr/neuralqa">Neural Question Answering</a>

                
              
                
                  
            





<a href="http://slides.com/suriyadeepanr/capsulenet">Capsule Network</a>

                
              
            </div>
          </li>
        
        
        
          <li>
            
            





<a href="http://suriya.strikingly.com/">Profile</a>

          </li>
        
        
      </ul>
    </div>

	
	<div class="avatar-container">
	  <div class="avatar-img-border">
	    <a href="http://suriyadeepan.github.io/">
	      <img class="avatar-img" src="Practical%20seq2seq_files/avatar-icon.png">
		</a>
	  </div>
	</div>
	

  </div>
</nav>


    <script src="Practical%20seq2seq_files/MathJax.js" id="">
</script>
  



<!-- TODO this file has become a mess, refactor it -->





<header class="header-section ">

<div class="intro-header no-img">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-heading">
          <h1>Practical seq2seq</h1>
		  
		    
			<h2 class="post-subheading">Revisiting sequence to sequence learning, with focus on implementation details</h2>
			
		  
		  
		  
		  <span class="post-meta">Posted on December 31, 2016</span>
		  
        </div>
      </div>
    </div>
  </div>
</div>
</header>






<div class="container">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
    	
      <article role="main" class="blog-post">
	      <p>In my <a href="http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/">last article</a>,
 I talked a bit about the theoretical aspect of the famous Sequence to 
Sequence Model. I have shared the code for my implementation of seq2seq -
 <a href="https://github.com/suriyadeepan/easy_seq2seq">easy_seq2seq</a>. I have adopted most of the code from <a href="https://www.tensorflow.org/tutorials/seq2seq/">en-fr translation</a>
 example provided by Google. Hence, most parts of the code, that dealt 
with data preprocessing, model evaluation were black boxes to me and to 
the readers. To make matters worse, the model trained on Cornell Movie 
Dialog corpus performed poorly. A lot of people complained about this. 
After training the model for days, most of the responses were gibberish.
 I apologize for wasting your time.</p>

<p>The objective of this article is two-fold; to provide the readers 
with a pre-trained model that actually works and to describe how to 
build and train such a model from (almost) scratch.</p>

<p>Before doing that, let us recap what we discussed in the previous article.</p>

<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>Recurrent Neural Networks or simple RNNs, are a special kind of 
neural networks that are capable of dealing with sequential data, like 
videos(sequence of frames) and more commonly, text sequences or 
basically any sequence of symbols. The beauty of it is, the network 
doesn’t need to know what the symbols mean. It will infer the meaning of
 symbols, by looking at the structure of the text and relative positions
 of symbols. There are some amazing articles on RNNs and what they are 
capable of. To put it simply, an RNN, unlike an MLP or CNN, has an 
internal state. Think of this state as memory of the network. As the RNN
 devours a sequence (sentence), word by word, the essential information 
about the sentence is maintained in this memory unit (internal state), 
which is periodically updated in each timestep. The essence of a 
sentence of 7 words will be captured by an RNN, in 7 timesteps. This is 
just a “story” about RNN, intended to provide a high level understanding
 of RNN. To understand the actual mechanism of RNN, read Denny Britz’s 
series of articles on RNN - <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">1</a>, <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/">2</a> ,<a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">3</a>, <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">4</a> and then move on to Karpathy’s <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNNs</a>.</p>

<p><img src="Practical%20seq2seq_files/rnn.jpg" alt=""> <br></p>

<p>The naive version of RNN, is typically called a <em>Vanilla</em> RNN,
 which is pretty pathetic in remembering long sequences. There are more 
complex versions of RNN, like LSTM (Long Short Term Memory) and GRU 
(Gated Recurrent Units) RNNs. The only difference between a Vanilla RNN 
and LSTM/GRU networks, is the architecture of the memory unit. An LSTM 
cell consists of multiple gates, for remembering useful information, 
forgetting unnecessary information and carefully exposing information at
 each time step. Christopher Olah does an amazing job explaining LSTM in
 this <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">article</a>. Check out Denny’s tutorial on <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">GRU</a>.</p>

<p><img src="Practical%20seq2seq_files/lstm.png" alt=""> <br></p>

<h2 id="sequence-to-sequence-learning">Sequence to Sequence Learning</h2>

<p>The Sequence to Sequence model (seq2seq) consists of two RNNs - an 
encoder and a decoder. The encoder reads the input sequence, word by 
word and emits a context (a function of final hidden state of encoder), 
which would ideally capture the essence (semantic summary) of the input 
sequence. Based on this context, the decoder generates the output 
sequence, one word at a time while looking at the context and the 
previous word during each timestep. This is a ridiculous 
oversimplification, but it gives you an idea of what happens in seq2seq.</p>

<p><img src="Practical%20seq2seq_files/seq2seq2.png" alt=""> <br></p>

<p>The context can be provided as the initial state of the decoder RNN 
or it can be connected to the hidden units at each time step. Now our 
objective is to jointly maximize the log probability of the output 
sequence conditioned on the input sequence.</p>

<p>Read these papers for a deeper understanding of seq2seq - <a href="http://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a>, <a href="http://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>, <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>, <a href="http://arxiv.org/abs/1506.05869">A Neural Conversational Model</a>.</p>

<h2 id="a-working-model">A Working Model</h2>

<p>As promised, here is a working model of a <a href="https://github.com/suriyadeepan/practical_seq2seq/blob/master/03-Twitter-chatbot.ipynb">twitter bot</a> based on <a href="https://github.com/suriyadeepan/practical_seq2seq/blob/master/seq2seq_wrapper.py">seq2seq</a> model. It is trained on <a href="https://github.com/Marsan-Ma/chat_corpus/blob/master/twitter_en.txt.gz">twitter chat log</a> borrowed from <a href="https://github.com/Marsan-Ma/">Marsan-Ma</a>. Before going into how to bootstrap and run the code, let us look at some of the decent responses spit out by the bot.</p>

<table>
  <thead>
    <tr>
      <th>Query</th>
      <th>Reply</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>donald trump won last nights presidential debate according to snap online polls</td>
      <td>thought he was a joke</td>
    </tr>
    <tr>
      <td>trump campaign please never delete this</td>
      <td>he was a joke</td>
    </tr>
    <tr>
      <td>s new scoop is unk the castro connection how trumps company violated the us embargo against cuba</td>
      <td>trump is a disgrace to the debate and the media is not a liar</td>
    </tr>
    <tr>
      <td>who won the first presidential debate</td>
      <td>trump will be a better time for a man</td>
    </tr>
    <tr>
      <td>just wanna live in unk everything is 10x better there</td>
      <td>i was just on the same side and i was like it was a good time</td>
    </tr>
    <tr>
      <td>the lil girl i forgot her name scares the shit out of me n probably always will lmaooo</td>
      <td>she was so cute and she was so cute and she was a bitch</td>
    </tr>
    <tr>
      <td>question of the day who do you think won last nights debate weigh in using</td>
      <td>hillary is a trump supporter and a woman who has a better job than a woman</td>
    </tr>
    <tr>
      <td>trump is a total embarrassment hes like a unk child who happens to be a professional liar and con man</td>
      <td>trump is a lot of money to get a great country</td>
    </tr>
    <tr>
      <td>chill the fuck out</td>
      <td>thats what i said</td>
    </tr>
    <tr>
      <td>so fun to see what the special effects team created for</td>
      <td>wow that is the best thing ive ever seen</td>
    </tr>
    <tr>
      <td>she was fine too unk voice</td>
      <td>she is a good guy</td>
    </tr>
    <tr>
      <td>the important ones are still here</td>
      <td>i dont think so</td>
    </tr>
    <tr>
      <td>been drinking pumpkin spice protein shake every morning for a week and now i literally cannot even</td>
      <td>me too i just got it</td>
    </tr>
    <tr>
      <td>lmao i just lost my job and i cant even leave early to be in the unk video today</td>
      <td>literally the same thing</td>
    </tr>
    <tr>
      <td>hey happy birthday have a nice day</td>
      <td>thank you</td>
    </tr>
  </tbody>
</table>

<p><em>For some reason people were really into tweeting about Trump.</em></p>

<p>Pretty good responses right? I have let the model overfit on the training data, just a little bit. The <em>unk</em>
 symbols in the conversations, refer to the words that aren’t frequent 
enough (rare) to be put in the vocabulary. Now, how to reproduce these 
results in your pc?</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># clone the repository
</span><br data-jekyll-commonmark-ghpages="">git clone <a class="vglnk" href="https://github.com/suriyadeepan/practical_seq2seq" rel="nofollow"><span>https</span><span>://</span><span>github</span><span>.</span><span>com</span><span>/</span><span>suriyadeepan</span><span>/</span><span>practical</span><span>_</span><span>seq2seq</span></a><br data-jekyll-commonmark-ghpages=""><span class="c"># pull the pretrained model
</span><br data-jekyll-commonmark-ghpages=""><span class="nb">cd </span>practical_seq2seq/ckpt/twitter/<br data-jekyll-commonmark-ghpages="">./pull <span class="c"># extract the archive
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># this will take some time ~830 MB
</span><br data-jekyll-commonmark-ghpages=""><span class="nb">cd</span> ../../<br data-jekyll-commonmark-ghpages=""><span class="c"># then you need the datasets to test the model
</span><br data-jekyll-commonmark-ghpages=""><span class="nb">cd </span>datasets/twitter/<br data-jekyll-commonmark-ghpages="">./pull <span class="c"># this wont take long
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># extract the archive
</span><br data-jekyll-commonmark-ghpages=""><span class="nb">cd</span> ../../<br data-jekyll-commonmark-ghpages=""><span class="c"># instead you could just test with your own queries
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  in that case you could just skip the step above
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># now.. open up jupyter notebook and run "03-Twitter-chatbot.ipynb"
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  the (pieces of) code is fairly intuitive
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#   figure out how to use it</span></code></pre></figure>

<p>Are you happy with the results? Is the pretrained model working well for you? If not, raise an issue in the <a href="https://github.com/suriyadeepan/practical_seq2seq">github repo</a>.</p>

<h2 id="understanding-the-code">Understanding the Code</h2>

<p>Let us move on to more interesting things. What are the steps 
involved in building such a bot? Any project in Machine Learning follows
 this pattern. Study and analyse the data, preprocess the data to make 
it compatible with the model, split it into train, valid, and test sets,
 create a model, feed the training data, let it overfit, reduce the 
depth/width of the model  and train again(which I usually skip), 
evaluate the model periodically to look for overfitting/underfitting.</p>

<h3 id="data-preprocessing">Data Preprocessing</h3>

<p>I keep all my datasets and preprocessing scripts in this repository - <a href="https://github.com/suriyadeepan/datasets">suriyadeepan/datasets</a>. Twitter dataset and scripts can be found <a href="https://github.com/suriyadeepan/datasets/tree/master/seq2seq/twitter">here</a>. Follow the instructions in <a href="https://github.com/suriyadeepan/datasets/blob/master/seq2seq/twitter/README.markdown">README</a>,
 to process raw data or to download processed data. I am presenting the 
gist of it in the snippet below. You might want to read <a href="https://github.com/suriyadeepan/datasets/blob/master/seq2seq/twitter/data.py">data.py</a> to get a clearer picture of preprocessing.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># read from file; return a list of sentences
</span><br data-jekyll-commonmark-ghpages=""><span class="k">def</span> <span class="nf">read_lines</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span><br data-jekyll-commonmark-ghpages="">    <span class="k">return</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><br data-jekyll-commonmark-ghpages=""><br data-jekyll-commonmark-ghpages=""><span class="c"># remove any character except alphanumerics
</span><br data-jekyll-commonmark-ghpages=""><span class="k">def</span> <span class="nf">filter_line</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">whitelist</span><span class="p">):</span><br data-jekyll-commonmark-ghpages="">    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">([</span> <span class="n">ch</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">line</span> <span class="k">if</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">whitelist</span> <span class="p">])</span><br data-jekyll-commonmark-ghpages=""><br data-jekyll-commonmark-ghpages=""><span class="c"># create loopup tables [word -&gt; index ] and [index -&gt; word]
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  frequency distribution of words in corpus
</span><br data-jekyll-commonmark-ghpages=""><span class="k">def</span> <span class="nf">index_</span><span class="p">(</span><span class="n">tokenized_sentences</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">):</span><br data-jekyll-commonmark-ghpages="">    <span class="c"># get frequency distribution
</span><br data-jekyll-commonmark-ghpages="">    <span class="n">freq_dist</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">FreqDist</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">tokenized_sentences</span><span class="p">))</span><br data-jekyll-commonmark-ghpages="">    <span class="c"># get vocabulary of 'vocab_size' most used words
</span><br data-jekyll-commonmark-ghpages="">    <span class="n">vocab</span> <span class="o">=</span> <span class="n">freq_dist</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span><br data-jekyll-commonmark-ghpages="">    <span class="c"># index2word
</span><br data-jekyll-commonmark-ghpages="">    <span class="n">index2word</span> <span class="o">=</span> <span class="p">[</span><span class="s">'_'</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">UNK</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">vocab</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages="">    <span class="c"># word2index
</span><br data-jekyll-commonmark-ghpages="">    <span class="n">word2index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">([(</span><span class="n">w</span><span class="p">,</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">index2word</span><span class="p">)]</span> <span class="p">)</span><br data-jekyll-commonmark-ghpages="">    <span class="k">return</span> <span class="n">index2word</span><span class="p">,</span> <span class="n">word2index</span><span class="p">,</span> <span class="n">freq_dist</span><br data-jekyll-commonmark-ghpages=""><br data-jekyll-commonmark-ghpages=""><span class="c"># other functions
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># def filter_data(sequences)
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  - filter dataset based on limits on length of sequences
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># def zero_pad(qtokenized, atokenized, word2index) 
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  - creates zero padded ndarrays
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># steps in involved in processing
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># read from file
</span><br data-jekyll-commonmark-ghpages=""><span class="n">lines</span> <span class="o">=</span> <span class="n">read_lines</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">FILENAME</span><span class="p">)</span> <br data-jekyll-commonmark-ghpages=""><span class="c"># change to lower case
</span><br data-jekyll-commonmark-ghpages=""><span class="n">lines</span> <span class="o">=</span> <span class="p">[</span> <span class="n">line</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="c"># filter out unnecessary characters
</span><br data-jekyll-commonmark-ghpages=""><span class="n">lines</span> <span class="o">=</span> <span class="p">[</span> <span class="n">filter_line</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">EN_WHITELIST</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="c"># filter out too long or too short sequences
</span><br data-jekyll-commonmark-ghpages=""><span class="n">qlines</span><span class="p">,</span> <span class="n">alines</span> <span class="o">=</span> <span class="n">filter_data</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># convert list of [lines of text] into list of [list of words ]
</span><br data-jekyll-commonmark-ghpages=""><span class="n">qtokenized</span> <span class="o">=</span> <span class="p">[</span> <span class="n">wordlist</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span> <span class="k">for</span> <span class="n">wordlist</span> <span class="ow">in</span> <span class="n">qlines</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="n">atokenized</span> <span class="o">=</span> <span class="p">[</span> <span class="n">wordlist</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span> <span class="k">for</span> <span class="n">wordlist</span> <span class="ow">in</span> <span class="n">alines</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="c"># indexing -&gt; idx2w, w2idx : en/ta
</span><br data-jekyll-commonmark-ghpages=""><span class="n">idx2w</span><span class="p">,</span> <span class="n">w2idx</span><span class="p">,</span> <span class="n">freq_dist</span> <span class="o">=</span> <span class="n">index_</span><span class="p">(</span> <span class="n">qtokenized</span> <span class="o">+</span> <span class="n">atokenized</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">VOCAB_SIZE</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># zero padding
</span><br data-jekyll-commonmark-ghpages=""><span class="n">idx_q</span><span class="p">,</span> <span class="n">idx_a</span> <span class="o">=</span> <span class="n">zero_pad</span><span class="p">(</span><span class="n">qtokenized</span><span class="p">,</span> <span class="n">atokenized</span><span class="p">,</span> <span class="n">w2idx</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># let us now save the necessary dictionaries
</span><br data-jekyll-commonmark-ghpages=""><span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span><br data-jekyll-commonmark-ghpages="">        <span class="s">'w2idx'</span> <span class="p">:</span> <span class="n">w2idx</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">        <span class="s">'idx2w'</span> <span class="p">:</span> <span class="n">idx2w</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">        <span class="s">'limit'</span> <span class="p">:</span> <span class="n">limit</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">        <span class="s">'freq_dist'</span> <span class="p">:</span> <span class="n">freq_dist</span><br data-jekyll-commonmark-ghpages="">            <span class="p">}</span><br data-jekyll-commonmark-ghpages=""><span class="c"># save metadata and data(idx_q, idx_a)
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  we haven't split the data into train/valid/test
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#   that comes later</span></code></pre></figure>

<p><strong>How to choose the vocabulary?</strong></p>

<p>It makes sense to have a vocabulary of the most frequent words. But 
how to choose the size of vocabulary? I have plotted the frequency vs 
words (most frequent to least frequent). I have chosen a vocabulary that
 covers most the area under the curve. Any word not in the vocabulary 
will be replaced with the UNK token.</p>

<p><img src="Practical%20seq2seq_files/freqdist.png" alt=""></p>

<p>The number of UNK tokens in the dataset make a big difference. If 
there are too many unknown tokens, the model learns to output UNK tokens
 more than the words in our limited vocabulary. I have reduced the 
overall percentage of unknown tokens in the dataset to 3%, by increasing
 the vocabulary size and by removing sentences with too many unknown 
words (rare words) from the dataset. It is advisable to keep it less 
than 5%.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># count of unknowns
</span><br data-jekyll-commonmark-ghpages=""><span class="n">unk_count</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_q</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">idx_a</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><br data-jekyll-commonmark-ghpages=""><span class="c"># count of words
</span><br data-jekyll-commonmark-ghpages=""><span class="n">word_count</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_q</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="p">(</span><span class="n">idx_a</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><br data-jekyll-commonmark-ghpages=""><span class="c"># % unknown
</span><br data-jekyll-commonmark-ghpages=""><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="si">% </span><span class="s">unknown : {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">unk_count</span><span class="o">/</span><span class="n">word_count</span><span class="p">)))</span></code></pre></figure>

<h3 id="feed_dict-helpers-data_utilspy">feed_dict helpers (data_utils.py)</h3>

<p>We have processed the data, changed it from raw lines of text stored 
in a file, to zero-padded numpy arrays of indices, along with necessary 
metadata (word2index and index2word dictionaries). Now it is time to 
write a few helper functions that will gather random examples from the 
dataset, in batches, to feed to the model for training and evaluation. <a href="https://github.com/suriyadeepan/practical_seq2seq/blob/master/data_utils.py">data_utils.py</a> has a couple of functions for that purpose.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># split the dataset into train/valid/test sets
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] )
</span><br data-jekyll-commonmark-ghpages=""><span class="c"># generate batches, by random sampling a bunch of items
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  yield (x_gen, y_gen)
</span><br data-jekyll-commonmark-ghpages=""><span class="k">def</span> <span class="nf">rand_batch_gen</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span><br data-jekyll-commonmark-ghpages="">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span><br data-jekyll-commonmark-ghpages="">        <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))),</span> <span class="n">batch_size</span><span class="p">)</span><br data-jekyll-commonmark-ghpages="">        <span class="k">yield</span> <span class="n">x</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">sample_idx</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><br data-jekyll-commonmark-ghpages=""><span class="c"># a generic decode function 
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  inputs : sequence, lookup
</span><br data-jekyll-commonmark-ghpages=""><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">lookup</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="s">''</span><span class="p">):</span> <br data-jekyll-commonmark-ghpages=""><span class="c"># 0 used for padding, is ignored
</span><br data-jekyll-commonmark-ghpages="">    <span class="k">return</span> <span class="n">separator</span><span class="o">.</span><span class="n">join</span><span class="p">([</span> <span class="n">lookup</span><span class="p">[</span><span class="n">element</span><span class="p">]</span> <br data-jekyll-commonmark-ghpages="">        <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">sequence</span> <span class="k">if</span> <span class="n">element</span> <span class="p">])</span></code></pre></figure>

<h3 id="build-a-wrapper-for-seq2seq">Build a wrapper for Seq2Seq</h3>

<p>I created a class, <a href="https://github.com/suriyadeepan/practical_seq2seq/blob/master/seq2seq_wrapper.py">Seq2Seq</a>
 that provides high level abstractions for building the graph, training,
 evaluation, saving and restoring trained model. The constructor takes 
these parameters as input:</p>

<ul>
  <li>xseq_len, yseq_len</li>
  <li>xvocab_size, yvocab_size</li>
  <li>emb_dim</li>
  <li>num_layers</li>
  <li>ckpt_path</li>
  <li>lr=0.0001</li>
  <li>epochs=100000</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># create an instance
</span><br data-jekyll-commonmark-ghpages=""><span class="n">model</span> <span class="o">=</span> <span class="n">seq2seq_wrapper</span><span class="o">.</span><span class="n">Seq2Seq</span><span class="p">(</span><span class="n">xseq_len</span><span class="o">=</span><span class="n">xseq_len</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">yseq_len</span><span class="o">=</span><span class="n">yseq_len</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">xvocab_size</span><span class="o">=</span><span class="n">xvocab_size</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">yvocab_size</span><span class="o">=</span><span class="n">yvocab_size</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">ckpt_path</span><span class="o">=</span><span class="s">'ckpt/twitter/'</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><br data-jekyll-commonmark-ghpages="">                               <span class="p">)</span></code></pre></figure>

<p>To build the graph, we create a bunch of placeholders for feeding 
input sequences, labels and decoder inputs. The labels correspond to the
 real output sequence. The decoder inputs start with a special ‘GO’ 
symbol, which in this case is just 0. For each sequence, we create a 
list of placeholders of datatype <em>tf.int64</em> (indices), of known length.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#  encoder inputs : list of indices of length xseq_len
</span><br data-jekyll-commonmark-ghpages=""><span class="bp">self</span><span class="o">.</span><span class="n">enc_ip</span> <span class="o">=</span> <span class="p">[</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,],</span> <br data-jekyll-commonmark-ghpages="">                <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <br data-jekyll-commonmark-ghpages="">                <span class="n">name</span><span class="o">=</span><span class="s">'ei_{}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xseq_len</span><span class="p">)</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  labels that represent the real outputs
</span><br data-jekyll-commonmark-ghpages=""><span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,],</span> <br data-jekyll-commonmark-ghpages="">                <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <br data-jekyll-commonmark-ghpages="">                <span class="n">name</span><span class="o">=</span><span class="s">'ei_{}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">yseq_len</span><span class="p">)</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  decoder inputs : 'GO' + [ y1, y2, ... y_t-1 ]
</span><br data-jekyll-commonmark-ghpages=""><span class="bp">self</span><span class="o">.</span><span class="n">dec_ip</span> <span class="o">=</span> <span class="p">[</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">enc_ip</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'GO'</span><span class="p">)</span> <span class="p">]</span> <br data-jekyll-commonmark-ghpages="">    <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span></code></pre></figure>

<p>Next, we create the LSTM cell, the most essential part of the graph. We define a placeholder for <em>keep_prob</em>, which will be used to control the dropout. <a href="http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/">Dropout</a>
 is a simple technique to prevent overfitting. It essentially just drops
 some of the unit activations in a layer, by making them zero. Then, we 
define a basic LSTM cell and then, wrap it with a DropoutWrapper. The 
LSTM cells, we just created are stacked together to form a stacked LSTM,
 using <em>MultiRNNCell</em>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Basic LSTM cell wrapped in Dropout Wrapper
</span><br data-jekyll-commonmark-ghpages=""><span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># define the basic cell
</span><br data-jekyll-commonmark-ghpages=""><span class="n">basic_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><br data-jekyll-commonmark-ghpages="">        <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span><br data-jekyll-commonmark-ghpages="">        <span class="n">output_keep_prob</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># stack cells together : n layered model
</span><br data-jekyll-commonmark-ghpages=""><span class="n">stacked_lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">rnn_cell</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">basic_cell</span><span class="p">]</span><span class="o">*</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></figure>

<p>Now we use a high level function - <em>embedding_rnn_seq2seq</em> 
provided by tensorflow’s seq2seq module, to create a seq2seq model, 
which does word embedding internally. A copy of the same model is 
created for testing, which uses the same parameters but has <em>feed_previous</em>
 switch enabled. This causes the decoder of the model to use the output 
of previous timestep as input to the current timestep, while during 
training, the input to a timestep (in the decoder) is taken from the 
labels sequence (the real output sequence).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="bp">self</span><span class="o">.</span><span class="n">decode_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode_states</span> <br data-jekyll-commonmark-ghpages="">	<span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">embedding_rnn_seq2seq</span><span class="p">(</span><br data-jekyll-commonmark-ghpages="">	<span class="bp">self</span><span class="o">.</span><span class="n">enc_ip</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">dec_ip</span><span class="p">,</span> <span class="n">stacked_lstm</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">	<span class="n">xvocab_size</span><span class="p">,</span> <span class="n">yvocab_size</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span></code></pre></figure>

<p>We use another high level function <em>sequence_loss</em>, to get the expression for loss. Then, we build a <em>train</em> operation that minimizes the loss.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">loss_weights</span> <span class="o">=</span> <span class="p">[</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <br data-jekyll-commonmark-ghpages="">	<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">seq2seq</span><span class="o">.</span><span class="n">sequence_loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decode_outputs</span><span class="p">,</span> <br data-jekyll-commonmark-ghpages="">	<span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">,</span> <span class="n">yvocab_size</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></code></pre></figure>

<p><strong>Training</strong> is fairly simple. We create a method <em>train</em>,
 that runs the train op, for a given number of epochs. Evaluation is 
done periodically on the validation set. The model is saved after 
evaluation. A dropout of 0.5 is used during training. The dropout is 
disabled during evaluation. When <em>restore_last_session</em> is called, the last saved checkpoint is restored and returned. The <em>predict</em> function does a forward step and returns the indices of most probable words emitted by the model. Read <a href="https://github.com/suriyadeepan/practical_seq2seq/blob/master/seq2seq_wrapper.py">seq2seq_wrapper.py</a> to get the big picture.</p>

<h2 id="training">Training</h2>

<p>Let us train the model using the processed dataset. The <em>load_data</em>
 function returns the dataset (x,y) and the metadata (index2word, 
word2index). We split the dataset into train, validation and test sets. 
We set the parameters of the model, like sequence length, vocabulary 
size and embedding dimensions. We then, create an instance of the model 
and train it, by passing data iterators to <em>model.train</em> method. 
If the training gets interrupted deliberately or otherwise, we can 
continue training from the last saved checkpoint. This is done by 
getting the session from <em>model.restore_last_session</em> and then passing the session to <em>model.train</em> method. The code for training is available <a href="https://github.com/suriyadeepan/practical_seq2seq/blob/master/03-Twitter-chatbot.py">here</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span><br data-jekyll-commonmark-ghpages=""><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span><br data-jekyll-commonmark-ghpages=""><span class="kn">import</span> <span class="nn">seq2seq_wrapper</span><br data-jekyll-commonmark-ghpages=""><span class="kn">from</span> <span class="nn">datasets.twitter</span> <span class="kn">import</span> <span class="n">data</span><br data-jekyll-commonmark-ghpages=""><span class="kn">import</span> <span class="nn">data_utils</span><br data-jekyll-commonmark-ghpages=""><span class="c"># load data from pickle and npy files
</span><br data-jekyll-commonmark-ghpages=""><span class="n">metadata</span><span class="p">,</span> <span class="n">idx_q</span><span class="p">,</span> <span class="n">idx_a</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">PATH</span><span class="o">=</span><span class="s">'datasets/twitter/'</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">),</span> <span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">testY</span><span class="p">),</span> <span class="p">(</span><span class="n">validX</span><span class="p">,</span> <span class="n">validY</span><span class="p">)</span> <span class="o">=</span> <span class="n">data_utils</span><span class="o">.</span><span class="n">split_dataset</span><span class="p">(</span><span class="n">idx_q</span><span class="p">,</span> <span class="n">idx_a</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># parameters 
</span><br data-jekyll-commonmark-ghpages=""><span class="n">xseq_len</span> <span class="o">=</span> <span class="n">trainX</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="n">yseq_len</span> <span class="o">=</span> <span class="n">trainY</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><br data-jekyll-commonmark-ghpages=""><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span><br data-jekyll-commonmark-ghpages=""><span class="n">xvocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s">'idx2w'</span><span class="p">])</span>  <br data-jekyll-commonmark-ghpages=""><span class="n">yvocab_size</span> <span class="o">=</span> <span class="n">xvocab_size</span><br data-jekyll-commonmark-ghpages=""><span class="n">emb_dim</span> <span class="o">=</span> <span class="mi">1024</span><br data-jekyll-commonmark-ghpages=""><span class="n">model</span> <span class="o">=</span> <span class="n">seq2seq_wrapper</span><span class="o">.</span><span class="n">Seq2Seq</span><span class="p">(</span><span class="n">xseq_len</span><span class="o">=</span><span class="n">xseq_len</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">yseq_len</span><span class="o">=</span><span class="n">yseq_len</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">xvocab_size</span><span class="o">=</span><span class="n">xvocab_size</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">yvocab_size</span><span class="o">=</span><span class="n">yvocab_size</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">ckpt_path</span><span class="o">=</span><span class="s">'ckpt/twitter/'</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">emb_dim</span><span class="o">=</span><span class="n">emb_dim</span><span class="p">,</span><br data-jekyll-commonmark-ghpages="">                               <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><br data-jekyll-commonmark-ghpages="">                               <span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="n">val_batch_gen</span> <span class="o">=</span> <span class="n">data_utils</span><span class="o">.</span><span class="n">rand_batch_gen</span><span class="p">(</span><span class="n">validX</span><span class="p">,</span> <span class="n">validY</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="n">train_batch_gen</span> <span class="o">=</span> <span class="n">data_utils</span><span class="o">.</span><span class="n">rand_batch_gen</span><span class="p">(</span><span class="n">trainX</span><span class="p">,</span> <span class="n">trainY</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c">#sess = model.restore_last_session()
</span><br data-jekyll-commonmark-ghpages=""><span class="n">sess</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_batch_gen</span><span class="p">,</span> <span class="n">val_batch_gen</span><span class="p">)</span><br data-jekyll-commonmark-ghpages=""><span class="c"># to continue training after interrupting,
</span><br data-jekyll-commonmark-ghpages=""><span class="c">#  run model.train with "sess=sess"</span></code></pre></figure>

<h2 id="more-datasets">More Datasets</h2>

<p>I have also trained the seq2seq model using other datasets, like <a href="https://github.com/suriyadeepan/datasets/tree/master/seq2seq/CMUdict">CMU Pronouncing Dictionary</a>, <a href="https://github.com/suriyadeepan/datasets/tree/master/seq2seq/cornell_movie_corpus">Cornell Movie Dialog Corpus</a>, and <a href="https://github.com/suriyadeepan/datasets/tree/master/seq2seq/en-ta-parallel-v2">Tamil to English parallel corpus</a>.
 As I am writing this article, my GTX960 is training the seq2seq model 
on Open Subtitles dataset. I will probably add the results of it 
tomorrow.</p>

<p><strong>CMU Pronouncing Dictionary</strong></p>

<p><img src="Practical%20seq2seq_files/phoneme.png" alt=""></p>

<p><strong>Cornell Movie Dialog Corpus</strong></p>

<table>
  <thead>
    <tr>
      <th>Query</th>
      <th>Reply</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>when are you going to let the police know</td>
      <td>you dont know what youre talking about</td>
    </tr>
    <tr>
      <td>you can do it</td>
      <td>yeah i think so</td>
    </tr>
    <tr>
      <td>wait are you saying that i dont appreciate</td>
      <td>i know you know what i mean</td>
    </tr>
    <tr>
      <td>youre terrible</td>
      <td>yeah but im not</td>
    </tr>
    <tr>
      <td>nah i dont think so</td>
      <td>why not</td>
    </tr>
    <tr>
      <td>hes pissing in our faces again and were just taking it</td>
      <td>i cant</td>
    </tr>
    <tr>
      <td>im sorry i dont know what else to say except im sorry</td>
      <td>why would you like to be</td>
    </tr>
    <tr>
      <td>nobodys gonna get you now get inside</td>
      <td>i cant go back</td>
    </tr>
    <tr>
      <td>shall i leave</td>
      <td>of course</td>
    </tr>
    <tr>
      <td>well i really think hes got a chance</td>
      <td>i know</td>
    </tr>
    <tr>
      <td>youd better be quiet sandy</td>
      <td>shut up</td>
    </tr>
    <tr>
      <td>jesus christ you scared the shit out of me</td>
      <td>whats going on</td>
    </tr>
    <tr>
      <td>well im sorry im really sorry ellie</td>
      <td>its okay</td>
    </tr>
    <tr>
      <td>my lady this play will end badly i will tell</td>
      <td>lets get out of here</td>
    </tr>
    <tr>
      <td>im going to miss you</td>
      <td>no youre not</td>
    </tr>
    <tr>
      <td>what do you mean</td>
      <td>i dont know i dont know what i mean</td>
    </tr>
    <tr>
      <td>my god these people are insane</td>
      <td>we dont know what they are</td>
    </tr>
    <tr>
      <td>this isnt a date</td>
      <td>no what is it</td>
    </tr>
    <tr>
      <td>you ought to go home and take care of that</td>
      <td>i cant do that</td>
    </tr>
    <tr>
      <td>is something wrong</td>
      <td>no no no</td>
    </tr>
  </tbody>
</table>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="http://suriyadeepan.github.io/2016-06-28-easy-seq2seq/">Chatbots with Seq2Seq</a></li>
  <li><a href="https://github.com/suriyadeepan/easy_seq2seq">easy_seq2seq</a></li>
  <li><a href="https://www.tensorflow.org/tutorials/seq2seq/">en-fr translation in Tensorflow</a></li>
  <li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Introduction to RNNs</a></li>
  <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of RNNs</a></li>
  <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM networks</a></li>
  <li><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">Implementing GRU</a></li>
  <li><a href="https://github.com/suriyadeepan/practical_seq2seq">practical_seq2seq</a></li>
  <li><a href="https://github.com/Marsan-Ma/chat_corpus/blob/master/twitter_en.txt.gz">Twitter Chat Log</a></li>
  <li><a href="https://github.com/suriyadeepan/datasets">suriyadeepan/datasets</a></li>
  <li><a href="https://github.com/suriyadeepan/datasets/tree/master/seq2seq/twitter">Twitter Dataset and Scripts</a></li>
</ul>

<p>I think I have discussed everything I wanted to discuss. If you are 
confused about anything we discussed, you are welcome to leave a comment
 below or raise an isssue in github. I will try my best to respond. I 
wanted to complete this article before the New Year. It is 11:35 PM here
 in Puducherry, India.</p>

<p><strong>இனிய புத்தாண்டு நல்வாழ்த்துக்கள்!!</strong></p>

	    </article>

<br>
 <!-- Tags -->
<ul class="tags">
  
    <li><a href="http://suriyadeepan.github.io/tags#tensorflow" class="tag">tensorflow</a></li>
  
    <li><a href="http://suriyadeepan.github.io/tags#machine%20learning" class="tag">machine learning</a></li>
  
    <li><a href="http://suriyadeepan.github.io/tags#seq2seq" class="tag">seq2seq</a></li>
  
    <li><a href="http://suriyadeepan.github.io/tags#NLP" class="tag">NLP</a></li>
  
    <li><a href="http://suriyadeepan.github.io/tags#chatbot" class="tag">chatbot</a></li>
  
</ul>  

	  	  
      <ul class="pager blog-pager">
        
        <li class="previous">
          <a href="http://suriyadeepan.github.io/2016-12-18-shining-ca/" data-toggle="tooltip" data-placement="top" title="All work and no play makes Jack a dull boy">← Previous Post</a>
        </li>
        


        
        <li class="next">
          <a href="http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/" data-toggle="tooltip" data-placement="top" title="Unfolding RNNs">Next Post →</a>
        </li>
        
      </ul>
      
      

	    
        <div class="disqus-comments">
	        
<div class="comments">
	<div id="disqus_thread"><iframe id="dsq-app5603" name="dsq-app5603" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1px !important; min-width: 100% !important; border: medium none !important; overflow: hidden !important; height: 6512px !important;" src="Practical%20seq2seq_files/a.html" horizontalscrolling="no" verticalscrolling="no" width="100%" frameborder="0"></iframe><iframe id="indicator-north" name="indicator-north" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 750px !important; border: medium none !important; overflow: hidden !important; top: 0px !important; min-width: 750px !important; max-width: 750px !important; position: fixed !important; z-index: 2147483646 !important; height: 19px !important; min-height: 19px !important; max-height: 19px !important; display: none !important;" frameborder="0"></iframe><iframe id="indicator-south" name="indicator-south" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 750px !important; border: medium none !important; overflow: hidden !important; bottom: 0px !important; min-width: 750px !important; max-width: 750px !important; position: fixed !important; z-index: 2147483646 !important; height: 19px !important; min-height: 19px !important; max-height: 19px !important; display: none !important;" frameborder="0"></iframe></div>
	<script type="text/javascript">
	    var disqus_shortname = 'suriyadeepan';
	    // ensure that pages with query string get the same discussion
            var url_parts = window.location.href.split("?");
            var disqus_url = url_parts[0];	    
	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();
	</script>
	<noscript>Please enable JavaScript to view the <a href="<a class="vglnk" href="http://disqus.com/?ref_noscript" rel="nofollow"><span>http</span><span>://</span><span>disqus</span><span>.</span><span>com</span><span>/?</span><span>ref</span><span>_</span><span>noscript</span></a>">comments powered by Disqus.</a></noscript>
</div>


        </div>
	    
    </div>
  </div>
</div>



    <footer>
  <div class="container beautiful-jekyll-footer">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          <li>
            <a href="https://www.facebook.com/suriya.geek.deepan" title="Facebook">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
          
          <li>
            <a href="https://github.com/suriyadeepan" title="GitHub">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="https://twitter.com/suriya_cosmist" title="Twitter">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
          <li>
            <a href="mailto:suriyadeepan.r@gmail.com" title="Email me">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
		  
		  
		  		  
        </ul>
        <p class="copyright text-muted">
		  Suriyadeepan Ramamoorthy
		  &nbsp;•&nbsp;
		  2017
		  
		  
		  &nbsp;•&nbsp;
		  <a href="http://suriyadeepan.github.io/">suriyadeepan.github.io</a>
		  
	    </p>
	        <!-- Please don't remove this, keep my open source work credited :) -->
		<p class="theme-by text-muted">
		  Theme by
		  <a href="http://deanattali.com/beautiful-jekyll/">beautiful-jekyll</a>
		</p>
      </div>
    </div>
  </div>
</footer>

  
    <!-- everything has to be repeated twice because on 2016-02-01 GitHub pages migrated to jekyll 3; see bug https://github.com/jekyll/jekyll/issues/4439 -->











  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script>
      	if (typeof jQuery == 'undefined') {
      	  document.write('<script src="/js/jquery-1.11.2.min.js"></scr' + 'ipt>');
      	}
      </script><script src="Practical%20seq2seq_files/jquery-1.js"></script><iframe style="display: none;"></iframe>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="Practical%20seq2seq_files/bootstrap.js"></script>
    
  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
	<script src="Practical%20seq2seq_files/main.js"></script>
    
  




	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-78656027-1', 'auto');
		ga('send', 'pageview');
	</script>
	<!-- End Google Analytics -->


  
  

</body></html>