<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
  <link rel="dns-prefetch" href="https://assets-cdn.github.com/">
  <link rel="dns-prefetch" href="https://avatars0.githubusercontent.com/">
  <link rel="dns-prefetch" href="https://avatars1.githubusercontent.com/">
  <link rel="dns-prefetch" href="https://avatars2.githubusercontent.com/">
  <link rel="dns-prefetch" href="https://avatars3.githubusercontent.com/">
  <link rel="dns-prefetch" href="https://github-cloud.s3.amazonaws.com/">
  <link rel="dns-prefetch" href="https://user-images.githubusercontent.com/">



  <link crossorigin="anonymous" media="all" integrity="sha512-Z0JAar9+DkI1NjGVdZr3GivARUgJtA0o2eHlTv7Ou2gshR5awWVf8QGsq11Ns9ZxQLEs+G5/SuARmvpOLMzulw==" rel="stylesheet" href="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/frameworks-95aff0b550d3fe338b645a4deebdcb1b.css">
  <link crossorigin="anonymous" media="all" integrity="sha512-E4SaFCOrA5CxlkiuvwhmxKCHYXPJXK1vtEhTYyhbKaqTXijIm/PBqbX71lj++xwfpESlg3vyRQLAIf6QuzQv1g==" rel="stylesheet" href="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/github-b1251687782606d65712e65e0c881315.css">
  
  
  
  

  <meta name="viewport" content="width=device-width">
  
  <title>tensorflow/nmt: TensorFlow Neural Machine Translation Tutorial</title>
    <meta name="description" content="TensorFlow Neural Machine Translation Tutorial">
    <link rel="search" type="application/opensearchdescription+xml" href="https://github.com/opensearch.xml" title="GitHub">
  <link rel="fluid-icon" href="https://github.com/fluidicon.png" title="GitHub">
  <meta property="fb:app_id" content="1401488693436528">

    
    <meta property="og:image" content="https://avatars2.githubusercontent.com/u/15658638?s=400&amp;v=4"><meta property="og:site_name" content="GitHub"><meta property="og:type" content="object"><meta property="og:title" content="tensorflow/nmt"><meta property="og:url" content="https://github.com/tensorflow/nmt"><meta property="og:description" content="nmt - TensorFlow Neural Machine Translation Tutorial">

  <link rel="assets" href="https://assets-cdn.github.com/">
  <link rel="web-socket" href="wss://live.github.com/_sockets/VjI6MzA1NjYzNTIwOmFlOGY5N2NjZWZkYTYzNjc0MzljZjNkZThjZDM0NjUzNGFiNTMwM2EwMWI4NjA1MTVhZWQ1MWIyYjNhYTA5YjQ=--d80e7750c10d8b7b68447c2c1b148758d44d86b9">
  <meta name="pjax-timeout" content="1000">
  <link rel="sudo-modal" href="https://github.com/sessions/sudo_modal">
  <meta name="request-id" content="7455:33F8:1AD8FA9:305F5EF:5B798DC8" data-pjax-transient="">


  

  <meta name="selected-link" value="repo_source" data-pjax-transient="">

    <meta name="google-site-verification" content="KT5gs8h0wvaagLKAVWq8bbeNwnZZK1r1XQysX3xurLU">
  <meta name="google-site-verification" content="ZzhVyEFwb7w3e0-uOTltm8Jsck2F5StVihD0exw2fsA">
  <meta name="google-site-verification" content="GXs5KoUUkNCoaAZn7wPN-t01Pywp9M3sEjnt_3_ZWPc">
    <meta name="google-analytics" content="UA-3769691-2">

<meta name="octolytics-host" content="collector.githubapp.com"><meta name="octolytics-app-id" content="github"><meta name="octolytics-event-url" content="https://collector.githubapp.com/github-external/browser_event"><meta name="octolytics-dimension-request_id" content="7455:33F8:1AD8FA9:305F5EF:5B798DC8"><meta name="octolytics-dimension-region_edge" content="iad"><meta name="octolytics-dimension-region_render" content="iad"><meta name="octolytics-actor-id" content="16961137"><meta name="octolytics-actor-login" content="saransh-mehta"><meta name="octolytics-actor-hash" content="f5ef41222440b09709820292b4815f68cb87e53b5f1bf097fd00d7dda9bd6528">
<meta name="analytics-location" content="/&lt;user-name&gt;/&lt;repo-name&gt;" data-pjax-transient="true">



  <meta class="js-ga-set" name="userId" content="3ab0b208581cc1025e04fa3e8e6d3921" %="">

<meta class="js-ga-set" name="dimension1" content="Logged In">


  

      <meta name="hostname" content="github.com">
    <meta name="user-login" content="saransh-mehta">

      <meta name="expected-hostname" content="github.com">
    <meta name="js-proxy-site-detection-payload" content="M2MxOWVjYmRlNTI4NTU1NmJhYWRiYWRiNmZjMDhjODUwMDZmMDZiNDk2NjY4ODcxMzUzZGM3NzZjNzQ3MGU5NHx7InJlbW90ZV9hZGRyZXNzIjoiMTY0LjEwMC4xMzEuNDAiLCJyZXF1ZXN0X2lkIjoiNzQ1NTozM0Y4OjFBRDhGQTk6MzA1RjVFRjo1Qjc5OERDOCIsInRpbWVzdGFtcCI6MTUzNDY5MjgxNSwiaG9zdCI6ImdpdGh1Yi5jb20ifQ==">

    <meta name="enabled-features" content="DASHBOARD_V2_LAYOUT_OPT_IN,EXPLORE_DISCOVER_REPOSITORIES,UNIVERSE_BANNER,FREE_TRIALS,MARKETPLACE_INSIGHTS,MARKETPLACE_DOCKERFILE_CI_CTA,MARKETPLACE_PLAN_RESTRICTION_EDITOR,MARKETPLACE_SEARCH,MARKETPLACE_INSIGHTS_CONVERSION_PERCENTAGES,MARKETPLACE_RETARGETING">

  <meta name="html-safe-nonce" content="d22774163558b3ec7c799996b32d9a3273bb13c9">

  <meta http-equiv="x-pjax-version" content="bd4c6adce17cfecaf4b73fa89fc41b21">
  

      <link href="https://github.com/tensorflow/nmt/commits/master.atom" rel="alternate" title="Recent Commits to nmt:master" type="application/atom+xml">

  <meta name="go-import" content="github.com/tensorflow/nmt git https://github.com/tensorflow/nmt.git">

  <meta name="octolytics-dimension-user_id" content="15658638"><meta name="octolytics-dimension-user_login" content="tensorflow"><meta name="octolytics-dimension-repository_id" content="95723115"><meta name="octolytics-dimension-repository_nwo" content="tensorflow/nmt"><meta name="octolytics-dimension-repository_public" content="true"><meta name="octolytics-dimension-repository_is_fork" content="false"><meta name="octolytics-dimension-repository_network_root_id" content="95723115"><meta name="octolytics-dimension-repository_network_root_nwo" content="tensorflow/nmt"><meta name="octolytics-dimension-repository_explore_github_marketplace_ci_cta_shown" content="false">


    <link rel="canonical" href="https://github.com/tensorflow/nmt" data-pjax-transient="">


  <meta name="browser-stats-url" content="https://api.github.com/_private/browser/stats">

  <meta name="browser-errors-url" content="https://api.github.com/_private/browser/errors">

  <link rel="mask-icon" href="https://assets-cdn.github.com/pinned-octocat.svg" color="#000000">
  <link rel="icon" type="image/x-icon" class="js-site-favicon" href="https://assets-cdn.github.com/favicon.ico">

<meta name="theme-color" content="#1e2327">



  <link rel="manifest" href="https://github.com/manifest.json" crossorigin="use-credentials">

  </head>

  <body class="logged-in env-production emoji-size-boost vsc-initialized">
    

  <div class="position-relative js-header-wrapper ">
    <a href="#start-of-content" tabindex="1" class="p-3 bg-blue text-white show-on-focus js-skip-to-content">Skip to content</a>
    <div id="js-pjax-loader-bar" class="pjax-loader-bar"><div class="progress"></div></div>

    
    
    



        
<header class="Header  f5" role="banner">
  <div class="d-flex flex-justify-between px-3 container-lg">
    <div class="d-flex flex-justify-between ">
      <div class="">
        <a class="header-logo-invertocat" href="https://github.com/" data-hotkey="g d" aria-label="Homepage" data-ga-click="Header, go to dashboard, icon:logo">
  <svg height="32" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="32" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg>
</a>

      </div>

    </div>

    <div class="HeaderMenu d-flex flex-justify-between flex-auto">
      <div class="d-flex">
            <div class="">
              <div class="header-search scoped-search site-scoped-search js-site-search position-relative js-jump-to" role="search combobox" aria-owns="jump-to-results" aria-label="Search or jump to" aria-haspopup="listbox" aria-expanded="true">
  <div class="position-relative">
    <!-- '"` --><!-- </textarea></xmp> --><form class="js-site-search-form" data-scope-type="Repository" data-scope-id="95723115" data-scoped-search-url="/tensorflow/nmt/search" data-unscoped-search-url="/search" action="/tensorflow/nmt/search" accept-charset="UTF-8" method="get"><input name="utf8" value="✓" type="hidden">
      <label class="form-control header-search-wrapper header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center js-chromeless-input-container">
        <input class="form-control header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable" data-hotkey="s,/" name="q" placeholder="Search or jump to…" data-unscoped-placeholder="Search or jump to…" data-scoped-placeholder="Search or jump to…" autocapitalize="off" aria-autocomplete="list" aria-controls="jump-to-results" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations#csrf-token=x1QGuxmb5Eh+JXNQKmAlxRH1nbUcKZ1S7o5Gmd9dBMyDWXVtfsaJKHYlzpAoo3zewgBYuV6FysaFXYfBS7TBZQ==" spellcheck="false" autocomplete="off" type="text">
          <input class="js-site-search-type-field" name="type" type="hidden">
            <img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/search-shortcut-hint.svg" alt="" class="mr-2 header-search-key-slash">

            <div class="Box position-absolute overflow-hidden d-none jump-to-suggestions js-jump-to-suggestions-container">
              <ul class="d-none js-jump-to-suggestions-template-container">
                <li class="d-flex flex-justify-start flex-items-center p-0 f5 navigation-item js-navigation-item">
                  <a tabindex="-1" class="no-underline d-flex flex-auto flex-items-center p-2 jump-to-suggestions-path js-jump-to-suggestion-path js-navigation-open" href="">
                    <div class="jump-to-octicon js-jump-to-octicon mr-2 text-center d-none"></div>
                    <img class="avatar mr-2 flex-shrink-0 js-jump-to-suggestion-avatar" alt="" aria-label="Team" src="" width="28" height="28">

                    <div class="jump-to-suggestion-name js-jump-to-suggestion-name flex-auto overflow-hidden text-left no-wrap css-truncate css-truncate-target">
                    </div>

                    <div class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none js-jump-to-badge-search">
                      <span class="js-jump-to-badge-search-text-default d-none" aria-label="in this repository">
                        In this repository
                      </span>
                      <span class="js-jump-to-badge-search-text-global d-none" aria-label="in all of GitHub">
                        All GitHub
                      </span>
                      <span aria-hidden="true" class="d-inline-block ml-1 v-align-middle">↵</span>
                    </div>

                    <div aria-hidden="true" class="border rounded-1 flex-shrink-0 bg-gray px-1 text-gray-light ml-1 f6 d-none d-on-nav-focus js-jump-to-badge-jump">
                      Jump to
                      <span class="d-inline-block ml-1 v-align-middle">↵</span>
                    </div>
                  </a>
                </li>
                <svg height="16" width="16" class="octicon octicon-repo flex-shrink-0 js-jump-to-repo-octicon-template" title="Repository" aria-label="Repository" viewBox="0 0 12 16" version="1.1" role="img"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"></path></svg>
                <svg height="16" width="16" class="octicon octicon-project flex-shrink-0 js-jump-to-project-octicon-template" title="Project" aria-label="Project" viewBox="0 0 15 16" version="1.1" role="img"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"></path></svg>
                <svg height="16" width="16" class="octicon octicon-search flex-shrink-0 js-jump-to-search-octicon-template" title="Search" aria-label="Search" viewBox="0 0 16 16" version="1.1" role="img"><path fill-rule="evenodd" d="M15.7 13.3l-3.81-3.83A5.93 5.93 0 0 0 13 6c0-3.31-2.69-6-6-6S1 2.69 1 6s2.69 6 6 6c1.3 0 2.48-.41 3.47-1.11l3.83 3.81c.19.2.45.3.7.3.25 0 .52-.09.7-.3a.996.996 0 0 0 0-1.41v.01zM7 10.7c-2.59 0-4.7-2.11-4.7-4.7 0-2.59 2.11-4.7 4.7-4.7 2.59 0 4.7 2.11 4.7 4.7 0 2.59-2.11 4.7-4.7 4.7z"></path></svg>
              </ul>
              <ul class="d-none js-jump-to-no-results-template-container">
                <li class="d-flex flex-justify-center flex-items-center p-3 f5 d-none">
                  <span class="text-gray">No suggested jump to results</span>
                </li>
              </ul>

              <ul id="jump-to-results" class="js-navigation-container jump-to-suggestions-results-container js-jump-to-suggestions-results-container">
                <li class="d-flex flex-justify-center flex-items-center p-0 f5">
                  <img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/octocat-spinner-128.gif" alt="Octocat Spinner Icon" class="m-2" width="28">
                </li>
              </ul>
            </div>
      </label>
</form>  </div>
</div>

            </div>

          <ul class="d-flex pl-2 flex-items-center text-bold list-style-none" role="navigation">
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-hotkey="g p" data-ga-click="Header, click, Nav menu - item:pulls context:user" aria-label="Pull requests you created" data-selected-links="/pulls /pulls/assigned /pulls/mentioned /pulls" href="https://github.com/pulls">
                Pull requests
</a>            </li>
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-hotkey="g i" data-ga-click="Header, click, Nav menu - item:issues context:user" aria-label="Issues you created" data-selected-links="/issues /issues/assigned /issues/mentioned /issues" href="https://github.com/issues">
                Issues
</a>            </li>
              <li>
                <a class="js-selected-navigation-item HeaderNavlink px-2" data-ga-click="Header, click, Nav menu - item:marketplace context:user" data-octo-click="marketplace_click" data-octo-dimensions="location:nav_bar" data-selected-links=" /marketplace" href="https://github.com/marketplace">
                   Marketplace
</a>              </li>
            <li>
              <a class="js-selected-navigation-item HeaderNavlink px-2" data-ga-click="Header, click, Nav menu - item:explore" data-selected-links="/explore /trending /trending/developers /integrations /integrations/feature/code /integrations/feature/collaborate /integrations/feature/ship showcases showcases_search showcases_landing /explore" href="https://github.com/explore">
                Explore
</a>            </li>
          </ul>
      </div>

      <div class="d-flex">
        
<ul class="user-nav d-flex flex-items-center list-style-none" id="user-links">
  <li class="dropdown">
    <span class="d-inline-block  px-2">
      
    <a aria-label="You have no unread notifications" class="notification-indicator tooltipped tooltipped-s  js-socket-channel js-notification-indicator" data-hotkey="g n" data-ga-click="Header, go to notifications, icon:read" data-channel="notification-changed:16961137" href="https://github.com/notifications">
        <span class="mail-status "></span>
        <svg class="octicon octicon-bell" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M13.99 11.991v1H0v-1l.73-.58c.769-.769.809-2.547 1.189-4.416.77-3.767 4.077-4.996 4.077-4.996 0-.55.45-1 .999-1 .55 0 1 .45 1 1 0 0 3.387 1.229 4.156 4.996.38 1.879.42 3.657 1.19 4.417l.659.58h-.01zM6.995 15.99c1.11 0 1.999-.89 1.999-1.999H4.996c0 1.11.89 1.999 1.999 1.999z"></path></svg>
</a>
    </span>
  </li>

  <li class="dropdown">
    <details class="details-overlay details-reset d-flex px-2 flex-items-center">
      <summary class="HeaderNavlink" aria-label="Create new…" data-ga-click="Header, create new, icon:add" aria-haspopup="menu">
        <svg class="octicon octicon-plus float-left mr-1 mt-1" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 9H7v5H5V9H0V7h5V2h2v5h5v2z"></path></svg>
        <span class="dropdown-caret mt-1"></span>
      </summary>
      <details-menu class="dropdown-menu dropdown-menu-sw" role="menu">
        
<a role="menuitem" class="dropdown-item" href="https://github.com/new" data-ga-click="Header, create new repository">
  New repository
</a>

  <a role="menuitem" class="dropdown-item" href="https://github.com/new/import" data-ga-click="Header, import a repository">
    Import repository
  </a>

<a role="menuitem" class="dropdown-item" href="https://gist.github.com/" data-ga-click="Header, create new gist">
  New gist
</a>

  <a role="menuitem" class="dropdown-item" href="https://github.com/organizations/new" data-ga-click="Header, create new organization">
    New organization
  </a>


  <div class="dropdown-divider"></div>
  <div class="dropdown-header">
    <span title="tensorflow/nmt">This repository</span>
  </div>
    <a role="menuitem" class="dropdown-item" href="https://github.com/tensorflow/nmt/issues/new" data-ga-click="Header, create new issue">
      New issue
    </a>

      </details-menu>
    </details>
  </li>

  <li class="dropdown">

    <details class="details-overlay details-reset d-flex pl-2 flex-items-center">
      <summary class="HeaderNavlink name mt-1" aria-label="View profile and more" data-ga-click="Header, show menu, icon:avatar" aria-haspopup="menu">
        <img alt="@saransh-mehta" class="avatar float-left mr-1" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/16961137.jpg" width="20" height="20">
        <span class="dropdown-caret"></span>
      </summary>
      <details-menu class="dropdown-menu dropdown-menu-sw" role="menu">
        <ul>
          <li class="header-nav-current-user css-truncate"><a role="menuitem" class="no-underline user-profile-link px-3 pt-2 pb-2 mb-n2 mt-n1 d-block" href="https://github.com/saransh-mehta" data-ga-click="Header, go to profile, text:Signed in as">Signed in as <strong class="css-truncate-target">saransh-mehta</strong></a></li>
          <li class="dropdown-divider"></li>
          <li><a role="menuitem" class="dropdown-item" href="https://github.com/saransh-mehta" data-ga-click="Header, go to profile, text:your profile">Your profile</a></li>
          <li><a role="menuitem" class="dropdown-item" href="https://github.com/saransh-mehta?tab=repositories" data-ga-click="Header, go to repositories, text:your repositories">Your repositories</a></li>
          <li><a role="menuitem" class="dropdown-item" href="https://github.com/saransh-mehta?tab=stars" data-ga-click="Header, go to starred repos, text:your stars">Your stars</a></li>
            <li><a role="menuitem" class="dropdown-item" href="https://gist.github.com/" data-ga-click="Header, your gists, text:your gists">Your gists</a></li>
          <li class="dropdown-divider"></li>
          <li><a role="menuitem" class="dropdown-item" href="https://help.github.com/" data-ga-click="Header, go to help, text:help">Help</a></li>
          <li><a role="menuitem" class="dropdown-item" href="https://github.com/settings/profile" data-ga-click="Header, go to settings, icon:settings">Settings</a></li>
          <li>
            <!-- '"` --><!-- </textarea></xmp> --><form class="logout-form" action="/logout" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="cQ8TjWiKJzB/RbE4fTPuSqB8/d4Xqrly0uIu7FZXNH6vYgId1gVmsCYiyho1P70eVCoVHBmbZrYtVrXkvZKWCA==" type="hidden">
              <button type="submit" class="dropdown-item dropdown-signout" data-ga-click="Header, sign out, icon:logout" role="menuitem">
                Sign out
              </button>
</form>          </li>
        </ul>
      </details-menu>
    </details>
  </li>
</ul>



        <!-- '"` --><!-- </textarea></xmp> --><form class="sr-only right-0" action="/logout" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="VOBVohHajBXAUQ6RC//mrgzzmDAHR9Vd1itI3OWVc+yKjUQyr1XNlZk2dbND87X6+KVw8gl2Cpkpn9PUDlDRmg==" type="hidden">
          <button type="submit" class="dropdown-item dropdown-signout" data-ga-click="Header, sign out, icon:logout">
            Sign out
          </button>
</form>      </div>
    </div>
  </div>
</header>

      

  </div>

  <div id="start-of-content" class="show-on-focus"></div>

    <div id="js-flash-container">


</div>



  <div role="main" class="application-main ">
        <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" class="">
    <div id="js-repo-pjax-container" data-pjax-container="">
      



  



  <div class="pagehead repohead instapaper_ignore readability-menu experiment-repo-nav  ">
    <div class="repohead-details-container clearfix container">

      <ul class="pagehead-actions">
  <li>
        <!-- '"` --><!-- </textarea></xmp> --><form data-autosubmit="true" data-remote="true" class="js-social-container" action="/notifications/subscribe" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="WiR2yJktMVrvdCayv+g7JvH347k3whK/lbmXQl0it8pUvG/NeuTaeMkdt96m1i/TnE3SQh/OVZpbJuskQ9mgUw==" type="hidden">      <input name="repository_id" id="repository_id" value="95723115" class="form-control" type="hidden">

        <div class="select-menu js-menu-container js-select-menu">
          <a href="https://github.com/tensorflow/nmt/subscription" class="btn btn-sm btn-with-count select-menu-button js-menu-target" role="button" aria-haspopup="true" aria-expanded="false" aria-label="Toggle repository notifications menu" data-ga-click="Repository, click Watch settings, action:files#disambiguate">
            <span class="js-select-button">
                <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"></path></svg>
                Watch
            </span>
          </a>
          <a class="social-count js-social-count" href="https://github.com/tensorflow/nmt/watchers" aria-label="205 users are watching this repository">
            205
          </a>

        <div class="select-menu-modal-holder">
          <div class="select-menu-modal subscription-menu-modal js-menu-content">
            <div class="select-menu-header js-navigation-enable" tabindex="-1">
              <svg class="octicon octicon-x js-menu-close" role="img" aria-label="Close" viewBox="0 0 12 16" version="1.1" width="12" height="16"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"></path></svg>
              <span class="select-menu-title">Notifications</span>
            </div>

              <div class="select-menu-list js-navigation-container" role="menu">

                <div class="select-menu-item js-navigation-item selected" role="menuitem" tabindex="0">
                  <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"></path></svg>
                  <div class="select-menu-item-text">
                    <input name="do" id="do_included" value="included" checked="checked" type="radio">
                    <span class="select-menu-item-heading">Not watching</span>
                    <span class="description">Be notified when participating or @mentioned.</span>
                    <span class="js-select-button-text hidden-select-button-text">
                      <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"></path></svg>
                      Watch
                    </span>
                  </div>
                </div>

                <div class="select-menu-item js-navigation-item " role="menuitem" tabindex="0">
                  <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"></path></svg>
                  <div class="select-menu-item-text">
                    <input name="do" id="do_subscribed" value="subscribed" type="radio">
                    <span class="select-menu-item-heading">Watching</span>
                    <span class="description">Be notified of all conversations.</span>
                    <span class="js-select-button-text hidden-select-button-text">
                      <svg class="octicon octicon-eye v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.06 2C3 2 0 8 0 8s3 6 8.06 6C13 14 16 8 16 8s-3-6-7.94-6zM8 12c-2.2 0-4-1.78-4-4 0-2.2 1.8-4 4-4 2.22 0 4 1.8 4 4 0 2.22-1.78 4-4 4zm2-4c0 1.11-.89 2-2 2-1.11 0-2-.89-2-2 0-1.11.89-2 2-2 1.11 0 2 .89 2 2z"></path></svg>
                        Unwatch
                    </span>
                  </div>
                </div>

                <div class="select-menu-item js-navigation-item " role="menuitem" tabindex="0">
                  <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"></path></svg>
                  <div class="select-menu-item-text">
                    <input name="do" id="do_ignore" value="ignore" type="radio">
                    <span class="select-menu-item-heading">Ignoring</span>
                    <span class="description">Never be notified.</span>
                    <span class="js-select-button-text hidden-select-button-text">
                      <svg class="octicon octicon-mute v-align-text-bottom" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 2.81v10.38c0 .67-.81 1-1.28.53L3 10H1c-.55 0-1-.45-1-1V7c0-.55.45-1 1-1h2l3.72-3.72C7.19 1.81 8 2.14 8 2.81zm7.53 3.22l-1.06-1.06-1.97 1.97-1.97-1.97-1.06 1.06L11.44 8 9.47 9.97l1.06 1.06 1.97-1.97 1.97 1.97 1.06-1.06L13.56 8l1.97-1.97z"></path></svg>
                        Stop ignoring
                    </span>
                  </div>
                </div>

              </div>

            </div>
          </div>
        </div>
</form>
  </li>

  <li>
    
  <div class="js-toggler-container js-social-container starring-container ">
    <!-- '"` --><!-- </textarea></xmp> --><form class="starred js-social-form" action="/tensorflow/nmt/unstar" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="RDey8cA8JhJ5TEBymCAEbYrCCyALoh8S1QR7aS0Oj/EWygGWuq1x2r+lcR+cFzJRZFNsOjiT75P45ds/iprnPg==" type="hidden">
      <input name="context" value="repository" type="hidden">
      <button type="submit" class="btn btn-sm btn-with-count js-toggler-target" aria-label="Unstar this repository" title="Unstar tensorflow/nmt" data-ga-click="Repository, click unstar button, action:files#disambiguate; text:Unstar">
        <svg class="octicon octicon-star v-align-text-bottom" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z"></path></svg>
        Unstar
      </button>
        <a class="social-count js-social-count" href="https://github.com/tensorflow/nmt/stargazers" aria-label="3359 users starred this repository">
          3,359
        </a>
</form>
    <!-- '"` --><!-- </textarea></xmp> --><form class="unstarred js-social-form" action="/tensorflow/nmt/star" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="i5LxrKUH/IWIygSbjyIIdOROLFdC7VHsd/Sp5V2e5oQE638GkMQVBhEj09dpEPBFF0acNJCt/WYgxMbcSCzZAA==" type="hidden">
      <input name="context" value="repository" type="hidden">
      <button type="submit" class="btn btn-sm btn-with-count js-toggler-target" aria-label="Star this repository" title="Star tensorflow/nmt" data-ga-click="Repository, click star button, action:files#disambiguate; text:Star">
        <svg class="octicon octicon-star v-align-text-bottom" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M14 6l-4.9-.64L7 1 4.9 5.36 0 6l3.6 3.26L2.67 14 7 11.67 11.33 14l-.93-4.74L14 6z"></path></svg>
        Star
      </button>
        <a class="social-count js-social-count" href="https://github.com/tensorflow/nmt/stargazers" aria-label="3359 users starred this repository">
          3,359
        </a>
</form>  </div>

  </li>

  <li>
          <!-- '"` --><!-- </textarea></xmp> --><form class="btn-with-count" action="/tensorflow/nmt/fork" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="a66GhoWjoM77fAqwLbtbx66TmsAa984/Dyfgnli8GnQ3gNgyVCF03L2wOMlAi3BmbgLUKkKtAGvFw6d2YS4JlQ==" type="hidden">
            <button type="submit" class="btn btn-sm btn-with-count" data-ga-click="Repository, show fork modal, action:files#disambiguate; text:Fork" title="Fork your own copy of tensorflow/nmt to your account" aria-label="Fork your own copy of tensorflow/nmt to your account">
              <svg class="octicon octicon-repo-forked v-align-text-bottom" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1a1.993 1.993 0 0 0-1 3.72V6L5 8 3 6V4.72A1.993 1.993 0 0 0 2 1a1.993 1.993 0 0 0-1 3.72V6.5l3 3v1.78A1.993 1.993 0 0 0 5 15a1.993 1.993 0 0 0 1-3.72V9.5l3-3V4.72A1.993 1.993 0 0 0 8 1zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3 10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm3-10c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"></path></svg>
              Fork
            </button>
</form>
    <a href="https://github.com/tensorflow/nmt/network/members" class="social-count" aria-label="1128 users forked this repository">
      1,128
    </a>
  </li>
</ul>

      <h1 class="public ">
  <svg class="octicon octicon-repo" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9H3V8h1v1zm0-3H3v1h1V6zm0-2H3v1h1V4zm0-2H3v1h1V2zm8-1v12c0 .55-.45 1-1 1H6v2l-1.5-1.5L3 16v-2H1c-.55 0-1-.45-1-1V1c0-.55.45-1 1-1h10c.55 0 1 .45 1 1zm-1 10H1v2h2v-1h3v1h5v-2zm0-10H2v9h9V1z"></path></svg>
  <span class="author" itemprop="author"><a class="url fn" rel="author" href="https://github.com/tensorflow">tensorflow</a></span><!--
--><span class="path-divider">/</span><!--
--><strong itemprop="name"><a data-pjax="#js-repo-pjax-container" href="https://github.com/tensorflow/nmt">nmt</a></strong>

</h1>

    </div>
    
<nav class="reponav js-repo-nav js-sidenav-container-pjax container" itemscope="" itemtype="http://schema.org/BreadcrumbList" role="navigation" data-pjax="#js-repo-pjax-container">

  <span itemscope="" itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a class="js-selected-navigation-item selected reponav-item" itemprop="url" data-hotkey="g c" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages /tensorflow/nmt" href="https://github.com/tensorflow/nmt">
      <svg class="octicon octicon-code" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M9.5 3L8 4.5 11.5 8 8 11.5 9.5 13 14 8 9.5 3zm-5 0L0 8l4.5 5L6 11.5 2.5 8 6 4.5 4.5 3z"></path></svg>
      <span itemprop="name">Code</span>
      <meta itemprop="position" content="1">
</a>  </span>

    <span itemscope="" itemtype="http://schema.org/ListItem" itemprop="itemListElement">
      <a itemprop="url" data-hotkey="g i" class="js-selected-navigation-item reponav-item" data-selected-links="repo_issues repo_labels repo_milestones /tensorflow/nmt/issues" href="https://github.com/tensorflow/nmt/issues">
        <svg class="octicon octicon-issue-opened" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg>
        <span itemprop="name">Issues</span>
        <span class="Counter">177</span>
        <meta itemprop="position" content="2">
</a>    </span>

  <span itemscope="" itemtype="http://schema.org/ListItem" itemprop="itemListElement">
    <a data-hotkey="g p" itemprop="url" class="js-selected-navigation-item reponav-item" data-selected-links="repo_pulls checks /tensorflow/nmt/pulls" href="https://github.com/tensorflow/nmt/pulls">
      <svg class="octicon octicon-git-pull-request" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M11 11.28V5c-.03-.78-.34-1.47-.94-2.06C9.46 2.35 8.78 2.03 8 2H7V0L4 3l3 3V4h1c.27.02.48.11.69.31.21.2.3.42.31.69v6.28A1.993 1.993 0 0 0 10 15a1.993 1.993 0 0 0 1-3.72zm-1 2.92c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zM4 3c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v6.56A1.993 1.993 0 0 0 2 15a1.993 1.993 0 0 0 1-3.72V4.72c.59-.34 1-.98 1-1.72zm-.8 10c0 .66-.55 1.2-1.2 1.2-.65 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2zM2 4.2C1.34 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"></path></svg>
      <span itemprop="name">Pull requests</span>
      <span class="Counter">21</span>
      <meta itemprop="position" content="3">
</a>  </span>

    <a data-hotkey="g b" class="js-selected-navigation-item reponav-item" data-selected-links="repo_projects new_repo_project repo_project /tensorflow/nmt/projects" href="https://github.com/tensorflow/nmt/projects">
      <svg class="octicon octicon-project" viewBox="0 0 15 16" version="1.1" width="15" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 12h3V2h-3v10zm-4-2h3V2H6v8zm-4 4h3V2H2v12zm-1 1h13V1H1v14zM14 0H1a1 1 0 0 0-1 1v14a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1V1a1 1 0 0 0-1-1z"></path></svg>
      Projects
      <span class="Counter">0</span>
</a>

    <a class="js-selected-navigation-item reponav-item" data-hotkey="g w" data-selected-links="repo_wiki /tensorflow/nmt/wiki" href="https://github.com/tensorflow/nmt/wiki">
      <svg class="octicon octicon-book" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M3 5h4v1H3V5zm0 3h4V7H3v1zm0 2h4V9H3v1zm11-5h-4v1h4V5zm0 2h-4v1h4V7zm0 2h-4v1h4V9zm2-6v9c0 .55-.45 1-1 1H9.5l-1 1-1-1H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h5.5l1 1 1-1H15c.55 0 1 .45 1 1zm-8 .5L7.5 3H2v9h6V3.5zm7-.5H9.5l-.5.5V12h6V3z"></path></svg>
      Wiki
</a>

  <a class="js-selected-navigation-item reponav-item" data-selected-links="repo_graphs repo_contributors dependency_graph pulse /tensorflow/nmt/pulse" href="https://github.com/tensorflow/nmt/pulse">
    <svg class="octicon octicon-graph" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 14v1H0V0h1v14h15zM5 13H3V8h2v5zm4 0H7V3h2v10zm4 0h-2V6h2v7z"></path></svg>
    Insights
</a>

</nav>


  </div>

<div class="container new-discussion-timeline experiment-repo-nav  ">
  <div class="repository-content ">

    
  

  <div class="js-repo-meta-container">
  <div class="repository-meta mb-0 mb-3 js-repo-meta-edit js-details-container ">
    <div class="repository-meta-content col-11 mb-1">
          <span class="col-11 text-gray-dark mr-2" itemprop="about">
            TensorFlow Neural Machine Translation Tutorial
          </span>
    </div>

  </div>

</div>



  <div class="overall-summary overall-summary-bottomless">
    <div class="stats-switcher-viewport js-stats-switcher-viewport">
      <div class="stats-switcher-wrapper">
      <ul class="numbers-summary">
        <li class="commits">
          <a data-pjax="" href="https://github.com/tensorflow/nmt/commits/master">
              <svg class="octicon octicon-history" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 13H6V6h5v2H8v5zM7 1C4.81 1 2.87 2.02 1.59 3.59L0 2v4h4L2.5 4.5C3.55 3.17 5.17 2.3 7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-.34.03-.67.09-1H.08C.03 7.33 0 7.66 0 8c0 3.86 3.14 7 7 7s7-3.14 7-7-3.14-7-7-7z"></path></svg>
              <span class="num text-emphasized">
                161
              </span>
              commits
          </a>
        </li>
        <li>
          <a data-pjax="" href="https://github.com/tensorflow/nmt/branches">
            <svg class="octicon octicon-git-branch" viewBox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 5c0-1.11-.89-2-2-2a1.993 1.993 0 0 0-1 3.72v.3c-.02.52-.23.98-.63 1.38-.4.4-.86.61-1.38.63-.83.02-1.48.16-2 .45V4.72a1.993 1.993 0 0 0-1-3.72C.88 1 0 1.89 0 3a2 2 0 0 0 1 1.72v6.56c-.59.35-1 .99-1 1.72 0 1.11.89 2 2 2 1.11 0 2-.89 2-2 0-.53-.2-1-.53-1.36.09-.06.48-.41.59-.47.25-.11.56-.17.94-.17 1.05-.05 1.95-.45 2.75-1.25S8.95 7.77 9 6.73h-.02C9.59 6.37 10 5.73 10 5zM2 1.8c.66 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2C1.35 4.2.8 3.65.8 3c0-.65.55-1.2 1.2-1.2zm0 12.41c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2zm6-8c-.66 0-1.2-.55-1.2-1.2 0-.65.55-1.2 1.2-1.2.65 0 1.2.55 1.2 1.2 0 .65-.55 1.2-1.2 1.2z"></path></svg>
            <span class="num text-emphasized">
              3
            </span>
            branches
          </a>
        </li>

        <li>
          <a href="https://github.com/tensorflow/nmt/releases">
            <svg class="octicon octicon-tag" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.685 1.72a2.49 2.49 0 0 0-1.76-.726H3.48A2.5 2.5 0 0 0 .994 3.48v2.456c0 .656.269 1.292.726 1.76l6.024 6.024a.99.99 0 0 0 1.402 0l4.563-4.563a.99.99 0 0 0 0-1.402L7.685 1.72zM2.366 7.048A1.54 1.54 0 0 1 1.9 5.925V3.48c0-.874.716-1.58 1.58-1.58h2.456c.418 0 .825.159 1.123.467l6.104 6.094-4.702 4.702-6.094-6.114zm.626-4.066h1.989v1.989H2.982V2.982h.01z"></path></svg>
            <span class="num text-emphasized">
              0
            </span>
            releases
          </a>
        </li>

        <li>
            <a href="https://github.com/tensorflow/nmt/graphs/contributors">
  <svg class="octicon octicon-organization" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 12.999c0 .439-.45 1-1 1H7.995c-.539 0-.994-.447-.995-.999H1c-.54 0-1-.561-1-1 0-2.634 3-4 3-4s.229-.409 0-1c-.841-.621-1.058-.59-1-3 .058-2.419 1.367-3 2.5-3s2.442.58 2.5 3c.058 2.41-.159 2.379-1 3-.229.59 0 1 0 1s1.549.711 2.42 2.088C9.196 9.369 10 8.999 10 8.999s.229-.409 0-1c-.841-.62-1.058-.59-1-3 .058-2.419 1.367-3 2.5-3s2.437.581 2.495 3c.059 2.41-.158 2.38-1 3-.229.59 0 1 0 1s3.005 1.366 3.005 4z"></path></svg>
    <span class="num text-emphasized">
      21
    </span>
    contributors
</a>

        </li>
          <li>
            <a href="https://github.com/tensorflow/nmt/blob/master/LICENSE">
              <svg class="octicon octicon-law" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7 4c-.83 0-1.5-.67-1.5-1.5S6.17 1 7 1s1.5.67 1.5 1.5S7.83 4 7 4zm7 6c0 1.11-.89 2-2 2h-1c-1.11 0-2-.89-2-2l2-4h-1c-.55 0-1-.45-1-1H8v8c.42 0 1 .45 1 1h1c.42 0 1 .45 1 1H3c0-.55.58-1 1-1h1c0-.55.58-1 1-1h.03L6 5H5c0 .55-.45 1-1 1H3l2 4c0 1.11-.89 2-2 2H2c-1.11 0-2-.89-2-2l2-4H1V5h3c0-.55.45-1 1-1h4c.55 0 1 .45 1 1h3v1h-1l2 4zM2.5 7L1 10h3L2.5 7zM13 10l-1.5-3-1.5 3h3z"></path></svg>
                Apache-2.0
            </a>
          </li>
      </ul>

        <div class="repository-lang-stats">
          <ol class="repository-lang-stats-numbers">
            <li>
                <a href="https://github.com/tensorflow/nmt/search?l=python" data-ga-click="Repository, language stats search click, location:repo overview">
                  <span class="color-block language-color" style="background-color:#3572A5;"></span>
                  <span class="lang">Python</span>
                  <span class="percent">97.0%</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/tensorflow/nmt/search?l=shell" data-ga-click="Repository, language stats search click, location:repo overview">
                  <span class="color-block language-color" style="background-color:#89e051;"></span>
                  <span class="lang">Shell</span>
                  <span class="percent">3.0%</span>
                </a>
            </li>
          </ol>
        </div>
      </div>
    </div>
  </div>

    <div class="repository-lang-stats-graph js-toggle-lang-stats" title="Click for language details" data-ga-click="Repository, language bar stats toggle, location:repo overview">
      <span class="language-color" aria-label="Python 97.0%" style="width:97.0%; background-color:#3572A5;" itemprop="keywords">Python</span>
      <span class="language-color" aria-label="Shell 3.0%" style="width:3.0%; background-color:#89e051;" itemprop="keywords">Shell</span>
    </div>



    

  <div class="file-navigation in-mid-page d-flex flex-items-start">
  
<div class="select-menu branch-select-menu js-menu-container js-select-menu float-left">
  <button class=" btn btn-sm select-menu-button js-menu-target css-truncate" data-hotkey="w" type="button" aria-label="Switch branches or tags" aria-expanded="false" aria-haspopup="true">
      <i>Branch:</i>
      <span class="js-select-button css-truncate-target">master</span>
  </button>

  <div class="select-menu-modal-holder js-menu-content js-navigation-container" data-pjax="">

    <div class="select-menu-modal">
      <div class="select-menu-header">
        <svg class="octicon octicon-x js-menu-close" role="img" aria-label="Close" viewBox="0 0 12 16" version="1.1" width="12" height="16"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"></path></svg>
        <span class="select-menu-title">Switch branches/tags</span>
      </div>

      <div class="select-menu-filters">
        <div class="select-menu-text-filter">
          <input aria-label="Filter branches/tags" id="context-commitish-filter-field" class="form-control js-filterable-field js-navigation-enable" placeholder="Filter branches/tags" type="text">
        </div>
        <div class="select-menu-tabs">
          <ul>
            <li class="select-menu-tab">
              <a href="#" data-tab-filter="branches" data-filter-placeholder="Filter branches/tags" class="js-select-menu-tab" role="tab">Branches</a>
            </li>
            <li class="select-menu-tab">
              <a href="#" data-tab-filter="tags" data-filter-placeholder="Find a tag…" class="js-select-menu-tab" role="tab">Tags</a>
            </li>
          </ul>
        </div>
      </div>

      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="branches" role="menu">

        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">


            <a class="select-menu-item js-navigation-item js-navigation-open selected" href="https://github.com/tensorflow/nmt/tree/master" data-name="master" data-skip-pjax="true" rel="nofollow">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"></path></svg>
              <span class="select-menu-item-text css-truncate-target js-select-menu-filter-text">
                master
              </span>
            </a>
            <a class="select-menu-item js-navigation-item js-navigation-open " href="https://github.com/tensorflow/nmt/tree/tf-1.2" data-name="tf-1.2" data-skip-pjax="true" rel="nofollow">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"></path></svg>
              <span class="select-menu-item-text css-truncate-target js-select-menu-filter-text">
                tf-1.2
              </span>
            </a>
            <a class="select-menu-item js-navigation-item js-navigation-open " href="https://github.com/tensorflow/nmt/tree/tf-1.4" data-name="tf-1.4" data-skip-pjax="true" rel="nofollow">
              <svg class="octicon octicon-check select-menu-item-icon" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M12 5l-8 8-4-4 1.5-1.5L4 10l6.5-6.5L12 5z"></path></svg>
              <span class="select-menu-item-text css-truncate-target js-select-menu-filter-text">
                tf-1.4
              </span>
            </a>
        </div>

          <div class="select-menu-no-results">Nothing to show</div>
      </div>

      <div class="select-menu-list select-menu-tab-bucket js-select-menu-tab-bucket" data-tab-filter="tags">
        <div data-filterable-for="context-commitish-filter-field" data-filterable-type="substring">


        </div>

        <div class="select-menu-no-results">Nothing to show</div>
      </div>

    </div>
  </div>
</div>


        <a href="https://github.com/tensorflow/nmt/pull/new/master" class="btn btn-sm new-pull-request-btn" data-pjax="" data-ga-click="Repository, new pull request, location:repo overview">
          New pull request
        </a>

  <div class="breadcrumb flex-auto">
    
  </div>

  <div class="BtnGroup">
      
  <!-- '"` --><!-- </textarea></xmp> --><form class="BtnGroup-form" action="/tensorflow/nmt/new/master" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="HLem1VtRYQBqAH7OYejAb0RyEOypFrA74vZXe6I4SYwbbiu0rqSFCUzy4rCDTGYIOx+MMimYzS/yvSp2iG99nw==" type="hidden">
    <button class="btn btn-sm BtnGroup-item" type="submit" data-disable-with="Creating file…">
      Create new file
    </button>
</form>

      
  <a href="https://github.com/tensorflow/nmt/upload/master" class="btn btn-sm BtnGroup-item">
    Upload files
  </a>


    <a href="https://github.com/tensorflow/nmt/find/master" class="btn btn-sm empty-icon float-right BtnGroup-item" data-pjax="" data-hotkey="t" data-ga-click="Repository, find file, location:repo overview">
      Find file
    </a>
  </div>
  

    <details class="get-repo-select-menu js-get-repo-select-menu position-relative details-overlay details-reset">
  <summary class="btn btn-sm btn-primary">
    Clone or download
    <span class="dropdown-caret"></span>
  </summary>
  <div class="position-relative">
    <div class="get-repo-modal dropdown-menu dropdown-menu-sw pb-0 js-toggler-container  js-get-repo-modal">

      <div class="get-repo-modal-options">
          <div class="clone-options https-clone-options">
              <!-- '"` --><!-- </textarea></xmp> --><form data-remote="true" action="/users/set_protocol?protocol_selector=ssh&amp;protocol_type=clone" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="5telN1XDKugLpVqZh9qq4OUdHtSQbpDCVMf5yPjqI+b4r9/RBizTPEqDdvwQ3eMMSDoEpvbnpMJ/Phw7w36+Bw==" type="hidden"><button type="submit" class="btn-link btn-change-protocol js-toggler-target float-right">Use SSH</button></form>

            <h4 class="mb-1">
              Clone with HTTPS
              <a class="muted-link" href="https://help.github.com/articles/which-remote-url-should-i-use" target="_blank" title="Which remote URL should I use?">
                <svg class="octicon octicon-question" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 10h2v2H6v-2zm4-3.5C10 8.64 8 9 8 9H6c0-.55.45-1 1-1h.5c.28 0 .5-.22.5-.5v-1c0-.28-.22-.5-.5-.5h-1c-.28 0-.5.22-.5.5V7H4c0-1.5 1.5-3 3-3s3 1 3 2.5zM7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7z"></path></svg>
              </a>
            </h4>
            <p class="mb-2 get-repo-decription-text">
              Use Git or checkout with SVN using the web URL.
            </p>

            <div class="input-group">
  <input class="form-control input-monospace input-sm js-url-field" value="https://github.com/tensorflow/nmt.git" aria-label="Clone this repository at https://github.com/tensorflow/nmt.git" readonly="readonly" type="text">
  <div class="input-group-button">
    <clipboard-copy value="https://github.com/tensorflow/nmt.git" aria-label="Copy to clipboard" class="btn btn-sm" tabindex="0" role="button">
      <svg class="octicon octicon-clippy" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2 13h4v1H2v-1zm5-6H2v1h5V7zm2 3V8l-3 3 3 3v-2h5v-2H9zM4.5 9H2v1h2.5V9zM2 12h2.5v-1H2v1zm9 1h1v2c-.02.28-.11.52-.3.7-.19.18-.42.28-.7.3H1c-.55 0-1-.45-1-1V4c0-.55.45-1 1-1h3c0-1.11.89-2 2-2 1.11 0 2 .89 2 2h3c.55 0 1 .45 1 1v5h-1V6H1v9h10v-2zM2 5h8c0-.55-.45-1-1-1H8c-.55 0-1-.45-1-1s-.45-1-1-1-1 .45-1 1-.45 1-1 1H3c-.55 0-1 .45-1 1z"></path></svg>
    </clipboard-copy>
  </div>
</div>

          </div>

          <div class="clone-options ssh-clone-options">
              <!-- '"` --><!-- </textarea></xmp> --><form data-remote="true" action="/users/set_protocol?protocol_selector=https&amp;protocol_type=clone" accept-charset="UTF-8" method="post"><input name="utf8" value="✓" type="hidden"><input name="authenticity_token" value="GALk1jBnK/ZC3vzHiKxDV+fmazH4ffw3OBXqL9ztF9UGep4wY4jSIgP40KIfqwq7SsFxQ570yDcT7A/c53mKNA==" type="hidden"><button type="submit" class="btn-link btn-change-protocol js-toggler-target float-right">Use HTTPS</button></form>

              <h4 class="mb-1">
                Clone with SSH
                <a class="muted-link" href="https://help.github.com/articles/which-remote-url-should-i-use" target="_blank" title="Which remote URL should I use?">
                  <svg class="octicon octicon-question" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 10h2v2H6v-2zm4-3.5C10 8.64 8 9 8 9H6c0-.55.45-1 1-1h.5c.28 0 .5-.22.5-.5v-1c0-.28-.22-.5-.5-.5h-1c-.28 0-.5.22-.5.5V7H4c0-1.5 1.5-3 3-3s3 1 3 2.5zM7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7z"></path></svg>
                </a>
              </h4>
              <p class="mb-2 get-repo-decription-text">
                Use an SSH key and passphrase from account.
              </p>

              <div class="input-group">
  <input class="form-control input-monospace input-sm js-url-field" value="git@github.com:tensorflow/nmt.git" aria-label="Clone this repository at git@github.com:tensorflow/nmt.git" readonly="readonly" type="text">
  <div class="input-group-button">
    <clipboard-copy value="git@github.com:tensorflow/nmt.git" aria-label="Copy to clipboard" class="btn btn-sm" tabindex="0" role="button">
      <svg class="octicon octicon-clippy" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2 13h4v1H2v-1zm5-6H2v1h5V7zm2 3V8l-3 3 3 3v-2h5v-2H9zM4.5 9H2v1h2.5V9zM2 12h2.5v-1H2v1zm9 1h1v2c-.02.28-.11.52-.3.7-.19.18-.42.28-.7.3H1c-.55 0-1-.45-1-1V4c0-.55.45-1 1-1h3c0-1.11.89-2 2-2 1.11 0 2 .89 2 2h3c.55 0 1 .45 1 1v5h-1V6H1v9h10v-2zM2 5h8c0-.55-.45-1-1-1H8c-.55 0-1-.45-1-1s-.45-1-1-1-1 .45-1 1-.45 1-1 1H3c-.55 0-1 .45-1 1z"></path></svg>
    </clipboard-copy>
  </div>
</div>

          </div>
        <div class="mt-2">
            <a href="https://desktop.github.com/" class="btn btn-outline get-repo-btn tooltipped tooltipped-s tooltipped-multiline js-get-repo" data-open-app="mac" aria-label="Clone tensorflow/nmt to your computer and use it in GitHub Desktop.">
    Open in Desktop
  </a>

<a href="https://github.com/tensorflow/nmt/archive/master.zip" class="btn btn-outline get-repo-btn
" rel="nofollow" data-ga-click="Repository, download zip, location:repo overview">
  Download ZIP
</a>

        </div>
      </div>

      <div class="js-modal-download-mac py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching GitHub Desktop<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://desktop.github.com/">download GitHub Desktop</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

      <div class="js-modal-download-windows py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching GitHub Desktop<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://desktop.github.com/">download GitHub Desktop</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

      <div class="js-modal-download-xcode py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching Xcode<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://developer.apple.com/xcode/">download Xcode</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

      <div class="js-modal-download-visual-studio py-2 px-3 d-none">
        <h4 class="lh-condensed mb-3">Launching Visual Studio<span class="animated-ellipsis-container"><span class="animated-ellipsis">...</span></span></h4>
        <p class="text-gray">If nothing happens, <a href="https://visualstudio.github.com/">download the GitHub extension for Visual Studio</a> and try again.</p>
        <p><button class="btn-link js-get-repo-modal-download-back">Go back</button></p>
      </div>

    </div>
  </div>
</details>

</div>


  


  <div class="commit-tease js-details-container Details">
    <span class="float-right">
      Latest commit
      <a class="commit-tease-sha" href="https://github.com/tensorflow/nmt/commit/365e7386e6659526f00fa4ad17eefb13d52e3706" data-pjax="">
        365e738
      </a>
      <span itemprop="dateModified"><relative-time datetime="2017-12-23T02:14:34Z" title="Dec 23, 2017, 7:44 AM GMT+5:30">on Dec 23, 2017</relative-time></span>
    </span>


      <div class="d-flex no-wrap">
        
<div class="AvatarStack flex-self-start ">
  <div class="AvatarStack-body tooltipped tooltipped-se tooltipped-align-left-1" aria-label="oahziur">

        <a href="https://github.com/oahziur" data-skip-pjax="true" class="avatar">
          <img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/4604464.png" alt="@oahziur" width="20" height="20">
        </a>
  </div>
</div>

        <div class="flex-auto f6">
          
      <a href="https://github.com/tensorflow/nmt/commits?author=oahziur" class="commit-author tooltipped tooltipped-s user-mention" aria-label="View all commits by oahziur">oahziur</a>


  committed
  <relative-time datetime="2017-12-23T02:14:34Z" title="Dec 23, 2017, 7:44 AM GMT+5:30">on Dec 23, 2017</relative-time>



      <a href="https://github.com/tensorflow/nmt/commit/365e7386e6659526f00fa4ad17eefb13d52e3706" class="message" data-pjax="true" title="Add option to use SampleEmbeddingHelper with a temperature.

PiperOrigin-RevId: 179980374">Add option to use SampleEmbeddingHelper with a temperature.</a>
        <span class="hidden-text-expander inline">
          <button type="button" class="ellipsis-expander js-details-target" aria-expanded="false">…</button>
        </span>

      <div class="commit-desc"><pre class="text-small">PiperOrigin-RevId: 179980374</pre></div>

        </div>
      </div>
  </div>




<div class="file-wrap">
  <a class="d-none js-permalink-shortcut" data-hotkey="y" href="https://github.com/tensorflow/nmt/tree/365e7386e6659526f00fa4ad17eefb13d52e3706">Permalink</a>

  <table class="files js-navigation-container js-active-navigation-container" data-pjax="">


    <tbody>
      <tr class="warning include-fragment-error">
        <td class="icon"><svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></td>
        <td class="content" colspan="3">Failed to load latest commit information.</td>
      </tr>

        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file-directory" viewBox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M13 4H7V3c0-.66-.31-1-1-1H1c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h12c.55 0 1-.45 1-1V5c0-.55-.45-1-1-1zM6 4H1V3h5v1z"></path></svg>
            <img class="spinner" alt="" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/octocat-spinner-32.gif" width="16" height="16">
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="nmt" id="f02b72cab4825557d7d297eed46c7294-486b2b5281d2da7eebb78c50d1f1cd5ce9bcb901" href="https://github.com/tensorflow/nmt/tree/master/nmt">nmt</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Add option to use SampleEmbeddingHelper with a temperature.

PiperOrigin-RevId: 179980374" class="message" href="https://github.com/tensorflow/nmt/commit/365e7386e6659526f00fa4ad17eefb13d52e3706">Add option to use SampleEmbeddingHelper with a temperature.</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-12-28T19:56:52Z" title="Dec 29, 2017, 1:26 AM GMT+5:30">8 months ago</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"></path></svg>
            <img class="spinner" alt="" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/octocat-spinner-32.gif" width="16" height="16">
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="CONTRIBUTING.md" id="6a3371457528722a734f3c51d9238c13-faa12a8b2ea81e4abc145c9828f2b2d0db2abeee" href="https://github.com/tensorflow/nmt/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="PiperOrigin-RevId: 157360504" class="message" href="https://github.com/tensorflow/nmt/commit/d8e6e8355ed9ad55b502e1292583ac55f0d7f756">PiperOrigin-RevId: 157360504</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-05-29T02:18:36Z" title="May 29, 2017, 7:48 AM GMT+5:30">a year ago</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"></path></svg>
            <img class="spinner" alt="" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/octocat-spinner-32.gif" width="16" height="16">
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="LICENSE" id="9879d6db96fd29134fc802214163b95a-d645695673349e3947e8e5ae42332d0ac3164cd7" itemprop="license" href="https://github.com/tensorflow/nmt/blob/master/LICENSE">LICENSE</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="PiperOrigin-RevId: 157360504" class="message" href="https://github.com/tensorflow/nmt/commit/d8e6e8355ed9ad55b502e1292583ac55f0d7f756">PiperOrigin-RevId: 157360504</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-05-29T02:18:36Z" title="May 29, 2017, 7:48 AM GMT+5:30">a year ago</time-ago></span>
          </td>
        </tr>
        <tr class="js-navigation-item">
          <td class="icon">
            <svg class="octicon octicon-file" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6 5H2V4h4v1zM2 8h7V7H2v1zm0 2h7V9H2v1zm0 2h7v-1H2v1zm10-7.5V14c0 .55-.45 1-1 1H1c-.55 0-1-.45-1-1V2c0-.55.45-1 1-1h7.5L12 4.5zM11 5L8 2H1v12h10V5z"></path></svg>
            <img class="spinner" alt="" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/octocat-spinner-32.gif" width="16" height="16">
          </td>
          <td class="content">
            <span class="css-truncate css-truncate-target"><a class="js-navigation-open" title="README.md" id="04c6e90faac2675aa89e2176d2eec7d8-a16ffbf9b361543dd56d33c61b2f6491100cb4aa" href="https://github.com/tensorflow/nmt/blob/master/README.md">README.md</a></span>
          </td>
          <td class="message">
            <span class="css-truncate css-truncate-target">
                  <a data-pjax="true" title="Correct other spelling issue

encoder_outpus -&gt; encoder_outputs

Merges #189

PiperOrigin-RevId: 177598034" class="message" href="https://github.com/tensorflow/nmt/commit/9be679427509d4a579a0ff8023ada67e32760986">Correct other spelling issue</a>
            </span>
          </td>
          <td class="age">
            <span class="css-truncate css-truncate-target"><time-ago datetime="2017-12-04T21:05:21Z" title="Dec 5, 2017, 2:35 AM GMT+5:30">9 months ago</time-ago></span>
          </td>
        </tr>
    </tbody>
  </table>

</div>




  <div id="readme" class="Box Box--condensed instapaper_body md">
    <div class="Box-header px-2 clearfix">
      <h3 class="Box-title pr-3">
        <svg class="octicon octicon-book" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M3 5h4v1H3V5zm0 3h4V7H3v1zm0 2h4V9H3v1zm11-5h-4v1h4V5zm0 2h-4v1h4V7zm0 2h-4v1h4V9zm2-6v9c0 .55-.45 1-1 1H9.5l-1 1-1-1H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h5.5l1 1 1-1H15c.55 0 1 .45 1 1zm-8 .5L7.5 3H2v9h6V3.5zm7-.5H9.5l-.5.5V12h6V3z"></path></svg>
        README.md
      </h3>
    </div>
      <div class="Box-body p-6">
        <article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-neural-machine-translation-seq2seq-tutorial" class="anchor" aria-hidden="true" href="#neural-machine-translation-seq2seq-tutorial"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Neural Machine Translation (seq2seq) Tutorial</h1>
<p><em>Authors: Thang Luong, Eugene Brevdo, Rui Zhao (<a href="https://research.googleblog.com/2017/07/building-your-own-neural-machine.html" rel="nofollow">Google Research Blogpost</a>, <a href="https://github.com/tensorflow/nmt">Github</a>)</em></p>
<p><em>This version of the tutorial requires <a href="https://github.com/tensorflow/tensorflow/#installation">TensorFlow Nightly</a>.
For using the stable TensorFlow versions, please consider other branches such as
<a href="https://github.com/tensorflow/nmt/tree/tf-1.4">tf-1.4</a>.</em></p>
<p><em>If make use of this codebase for your research, please cite
<a href="#bibtex">this</a>.</em></p>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#basic">Basic</a>
<ul>
<li><a href="#background-on-neural-machine-translation">Background on Neural Machine Translation</a></li>
<li><a href="#installing-the-tutorial">Installing the Tutorial</a></li>
<li><a href="#training--how-to-build-our-first-nmt-system">Training – <em>How to build our first NMT system</em></a>
<ul>
<li><a href="#embedding">Embedding</a></li>
<li><a href="#encoder">Encoder</a></li>
<li><a href="#decoder">Decoder</a></li>
<li><a href="#loss">Loss</a></li>
<li><a href="#gradient-computation--optimization">Gradient computation &amp; optimization</a></li>
</ul>
</li>
<li><a href="#hands-on--lets-train-an-nmt-model">Hands-on – <em>Let's train an NMT model</em></a></li>
<li><a href="#inference--how-to-generate-translations">Inference – <em>How to generate translations</em></a></li>
</ul>
</li>
<li><a href="#intermediate">Intermediate</a>
<ul>
<li><a href="#background-on-the-attention-mechanism">Background on the Attention Mechanism</a></li>
<li><a href="#attention-wrapper-api">Attention Wrapper API</a></li>
<li><a href="#hands-on--building-an-attention-based-nmt-model">Hands-on – <em>Building an attention-based NMT model</em></a></li>
</ul>
</li>
<li><a href="#tips--tricks">Tips &amp; Tricks</a>
<ul>
<li><a href="#building-training-eval-and-inference-graphs">Building Training, Eval, and Inference Graphs</a></li>
<li><a href="#data-input-pipeline">Data Input Pipeline</a></li>
<li><a href="#other-details-for-better-nmt-models">Other details for better NMT models</a>
<ul>
<li><a href="#bidirectional-rnns">Bidirectional RNNs</a></li>
<li><a href="#beam-search">Beam search</a></li>
<li><a href="#hyperparameters">Hyperparameters</a></li>
<li><a href="#multi-gpu-training">Multi-GPU training</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a>
<ul>
<li><a href="#iwslt-english-vietnamese">IWSLT English-Vietnamese</a></li>
<li><a href="#wmt-german-english">WMT German-English</a></li>
<li><a href="#wmt-english-german--full-comparison">WMT English-German — <em>Full Comparison</em></a></li>
<li><a href="#standard-hparams">Standard HParams</a></li>
</ul>
</li>
<li><a href="#other-resources">Other resources</a></li>
<li><a href="#acknowledgment">Acknowledgment</a></li>
<li><a href="#references">References</a></li>
<li><a href="#bibtex">BibTex</a></li>
</ul>
<h1><a id="user-content-introduction" class="anchor" aria-hidden="true" href="#introduction"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h1>
<p>Sequence-to-sequence (seq2seq) models
(<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow">Sutskever et al., 2014</a>,
<a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf" rel="nofollow">Cho et al., 2014</a>) have
enjoyed great success in a variety of tasks such as machine translation, speech
recognition, and text summarization. This tutorial gives readers a full
understanding of seq2seq models and shows how to build a competitive seq2seq
model from scratch. We focus on the task of Neural Machine Translation (NMT)
which was the very first testbed for seq2seq models with
wild
<a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html" rel="nofollow">success</a>. The
included code is lightweight, high-quality, production-ready, and incorporated
with the latest research ideas. We achieve this goal by:</p>
<ol>
<li>Using the recent decoder / attention
wrapper
<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq/python/ops">API</a>,
TensorFlow 1.2 data iterator</li>
<li>Incorporating our strong expertise in building recurrent and seq2seq models</li>
<li>Providing tips and tricks for building the very best NMT models and replicating
<a href="https://research.google.com/pubs/pub45610.html" rel="nofollow">Google’s NMT (GNMT) system</a>.</li>
</ol>
<p>We believe that it is important to provide benchmarks that people can easily
replicate. As a result, we have provided full experimental results and
pretrained on models on the following publicly available datasets:</p>
<ol>
<li><em>Small-scale</em>: English-Vietnamese parallel corpus of TED talks (133K sentence
pairs) provided by
the
<a href="https://sites.google.com/site/iwsltevaluation2015/" rel="nofollow">IWSLT Evaluation Campaign</a>.</li>
<li><em>Large-scale</em>: German-English parallel corpus (4.5M sentence pairs) provided
by the <a href="http://www.statmt.org/wmt16/translation-task.html" rel="nofollow">WMT Evaluation Campaign</a>.</li>
</ol>
<p>We first build up some basic knowledge about seq2seq models for NMT, explaining
how to build and train a vanilla NMT model. The second part will go into details
of building a competitive NMT model with attention mechanism. We then discuss
tips and tricks to build the best possible NMT models (both in speed and
translation quality) such as TensorFlow best practices (batching, bucketing),
bidirectional RNNs, beam search, as well as scaling up to multiple GPUs using GNMT attention.</p>
<h1><a id="user-content-basic" class="anchor" aria-hidden="true" href="#basic"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Basic</h1>
<h2><a id="user-content-background-on-neural-machine-translation" class="anchor" aria-hidden="true" href="#background-on-neural-machine-translation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Background on Neural Machine Translation</h2>
<p>Back in the old days, traditional phrase-based translation systems performed
their task by breaking up source sentences into multiple chunks and then
translated them phrase-by-phrase. This led to disfluency in the translation
outputs and was not quite like how we, humans, translate. We read the entire
source sentence, understand its meaning, and then produce a translation. Neural
Machine Translation (NMT) mimics that!</p>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/encdec.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/encdec.jpg" style="max-width:100%;" width="80%"></a>
<br>
Figure 1. <b>Encoder-decoder architecture</b> – example of a general approach for
NMT. An encoder converts a source sentence into a "meaning" vector which is
passed through a <i>decoder</i> to produce a translation.
</p>
<p>Specifically, an NMT system first reads the source sentence using an <em>encoder</em>
to build
a
<a href="https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence" rel="nofollow">"thought" vector</a>,
a sequence of numbers that represents the sentence meaning; a <em>decoder</em>, then,
processes the sentence vector to emit a translation, as illustrated in
Figure 1. This is often referred to as the <em>encoder-decoder architecture</em>. In
this manner, NMT addresses the local translation problem in the traditional
phrase-based approach: it can capture <em>long-range dependencies</em> in languages,
e.g., gender agreements; syntax structures; etc., and produce much more fluent
translations as demonstrated
by
<a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html" rel="nofollow">Google Neural Machine Translation systems</a>.</p>
<p>NMT models vary in terms of their exact architectures. A natural choice for
sequential data is the recurrent neural network (RNN), used by most NMT models.
Usually an RNN is used for both the encoder and decoder. The RNN models,
however, differ in terms of: (a) <em>directionality</em> – unidirectional or
bidirectional; (b) <em>depth</em> – single- or multi-layer; and (c) <em>type</em> – often
either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit
(GRU). Interested readers can find more information about RNNs and LSTM on
this <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow">blog post</a>.</p>
<p>In this tutorial, we consider as examples a <em>deep multi-layer RNN</em> which is
unidirectional and uses LSTM as a recurrent unit. We show an example of such a
model in Figure 2. In this example, we build a model to translate a source
sentence "I am a student" into a target sentence "Je suis étudiant". At a high
level, the NMT model consists of two recurrent neural networks: the <em>encoder</em>
RNN simply consumes the input source words without making any prediction; the
<em>decoder</em>, on the other hand, processes the target sentence while predicting the
next words.</p>
<p>For more information, we refer readers
to <a href="https://github.com/lmthang/thesis">Luong (2016)</a> which this tutorial is
based on.</p>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/seq2seq.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/seq2seq.jpg" style="max-width:100%;" width="48%"></a>
<br>
Figure 2. <b>Neural machine translation</b> – example of a deep recurrent
architecture proposed by for translating a source sentence "I am a student" into
a target sentence "Je suis étudiant". Here, "&lt;s&gt;" marks the start of the
decoding process while "&lt;/s&gt;" tells the decoder to stop.
</p>
<h2><a id="user-content-installing-the-tutorial" class="anchor" aria-hidden="true" href="#installing-the-tutorial"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installing the Tutorial</h2>
<p>To install this tutorial, you need to have TensorFlow installed on your system.
This tutorial requires TensorFlow Nightly. To install TensorFlow, follow
the <a href="https://www.tensorflow.org/install/" rel="nofollow">installation instructions here</a>.</p>
<p>Once TensorFlow is installed, you can download the source code of this tutorial
by running:</p>
<div class="highlight highlight-source-shell"><pre>git clone https://github.com/tensorflow/nmt/</pre></div>
<h2><a id="user-content-training--how-to-build-our-first-nmt-system" class="anchor" aria-hidden="true" href="#training--how-to-build-our-first-nmt-system"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Training – How to build our first NMT system</h2>
<p>Let's first dive into the heart of building an NMT model with concrete code
snippets through which we will explain Figure 2 in more detail. We defer data
preparation and the full code to later. This part refers to
file
<a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py"><strong>model.py</strong></a>.</p>
<p>At the bottom layer, the encoder and decoder RNNs receive as input the
following: first, the source sentence, then a boundary marker "&lt;s&gt;" which
indicates the transition from the encoding to the decoding mode, and the target
sentence.  For <em>training</em>, we will feed the system with the following tensors,
which are in time-major format and contain word indices:</p>
<ul>
<li><strong>encoder_inputs</strong> [max_encoder_time, batch_size]: source input words.</li>
<li><strong>decoder_inputs</strong> [max_decoder_time, batch_size]: target input words.</li>
<li><strong>decoder_outputs</strong> [max_decoder_time, batch_size]: target output words,
these are decoder_inputs shifted to the left by one time step with an
end-of-sentence tag appended on the right.</li>
</ul>
<p>Here for efficiency, we train with multiple sentences (batch_size) at
once. Testing is slightly different, so we will discuss it later.</p>
<h3><a id="user-content-embedding" class="anchor" aria-hidden="true" href="#embedding"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Embedding</h3>
<p>Given the categorical nature of words, the model must first look up the source
and target embeddings to retrieve the corresponding word representations. For
this <em>embedding layer</em> to work, a vocabulary is first chosen for each language.
Usually, a vocabulary size V is selected, and only the most frequent V words are
treated as unique.  All other words are converted to an "unknown" token and all
get the same embedding.  The embedding weights, one set per language, are
usually learned during training.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Embedding</span>
embedding_encoder <span class="pl-k">=</span> variable_scope.get_variable(
    <span class="pl-s"><span class="pl-pds">"</span>embedding_encoder<span class="pl-pds">"</span></span>, [src_vocab_size, embedding_size], <span class="pl-c1">...</span>)
<span class="pl-c"><span class="pl-c">#</span> Look up embedding:</span>
<span class="pl-c"><span class="pl-c">#</span>   encoder_inputs: [max_time, batch_size]</span>
<span class="pl-c"><span class="pl-c">#</span>   encoder_emb_inp: [max_time, batch_size, embedding_size]</span>
encoder_emb_inp <span class="pl-k">=</span> embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)</pre></div>
<p>Similarly, we can build <em>embedding_decoder</em> and <em>decoder_emb_inp</em>. Note that one
can choose to initialize embedding weights with pretrained word representations
such as word2vec or Glove vectors. In general, given a large amount of training
data we can learn these embeddings from scratch.</p>
<h3><a id="user-content-encoder" class="anchor" aria-hidden="true" href="#encoder"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Encoder</h3>
<p>Once retrieved, the word embeddings are then fed as input into the main network,
which consists of two multi-layer RNNs – an encoder for the source language and
a decoder for the target language. These two RNNs, in principle, can share the
same weights; however, in practice, we often use two different RNN parameters
(such models do a better job when fitting large training datasets). The
<em>encoder</em> RNN uses zero vectors as its starting states and is built as follows:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Build RNN cell</span>
encoder_cell <span class="pl-k">=</span> tf.nn.rnn_cell.BasicLSTMCell(num_units)

<span class="pl-c"><span class="pl-c">#</span> Run Dynamic RNN</span>
<span class="pl-c"><span class="pl-c">#</span>   encoder_outputs: [max_time, batch_size, num_units]</span>
<span class="pl-c"><span class="pl-c">#</span>   encoder_state: [batch_size, num_units]</span>
encoder_outputs, encoder_state <span class="pl-k">=</span> tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    <span class="pl-v">sequence_length</span><span class="pl-k">=</span>source_sequence_length, <span class="pl-v">time_major</span><span class="pl-k">=</span><span class="pl-c1">True</span>)</pre></div>
<p>Note that sentences have different lengths to avoid wasting computation, we tell
<em>dynamic_rnn</em> the exact source sentence lengths through
<em>source_sequence_length</em>. Since our input is time major, we set
<em>time_major=True</em>. Here, we build only a single layer LSTM, <em>encoder_cell</em>. We
will describe how to build multi-layer LSTMs, add dropout, and use attention in
a later section.</p>
<h3><a id="user-content-decoder" class="anchor" aria-hidden="true" href="#decoder"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Decoder</h3>
<p>The <em>decoder</em> also needs to have access to the source information, and one
simple way to achieve that is to initialize it with the last hidden state of the
encoder, <em>encoder_state</em>. In Figure 2, we pass the hidden state at the source
word "student" to the decoder side.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Build RNN cell</span>
decoder_cell <span class="pl-k">=</span> tf.nn.rnn_cell.BasicLSTMCell(num_units)</pre></div>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Helper</span>
helper <span class="pl-k">=</span> tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, decoder_lengths, <span class="pl-v">time_major</span><span class="pl-k">=</span><span class="pl-c1">True</span>)
<span class="pl-c"><span class="pl-c">#</span> Decoder</span>
decoder <span class="pl-k">=</span> tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    <span class="pl-v">output_layer</span><span class="pl-k">=</span>projection_layer)
<span class="pl-c"><span class="pl-c">#</span> Dynamic decoding</span>
outputs, _ <span class="pl-k">=</span> tf.contrib.seq2seq.dynamic_decode(decoder, <span class="pl-c1">...</span>)
logits <span class="pl-k">=</span> outputs.rnn_output</pre></div>
<p>Here, the core part of this code is the <em>BasicDecoder</em> object, <em>decoder</em>, which
receives <em>decoder_cell</em> (similar to encoder_cell), a <em>helper</em>, and the previous
<em>encoder_state</em> as inputs. By separating out decoders and helpers, we can reuse
different codebases, e.g., <em>TrainingHelper</em> can be substituted with
<em>GreedyEmbeddingHelper</em> to do greedy decoding. See more
in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/helper.py">helper.py</a>.</p>
<p>Lastly, we haven't mentioned <em>projection_layer</em> which is a dense matrix to turn
the top hidden states to logit vectors of dimension V. We illustrate this
process at the top of Figure 2.</p>
<div class="highlight highlight-source-python"><pre>projection_layer <span class="pl-k">=</span> layers_core.Dense(
    tgt_vocab_size, <span class="pl-v">use_bias</span><span class="pl-k">=</span><span class="pl-c1">False</span>)</pre></div>
<h3><a id="user-content-loss" class="anchor" aria-hidden="true" href="#loss"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Loss</h3>
<p>Given the <em>logits</em> above, we are now ready to compute our training loss:</p>
<div class="highlight highlight-source-python"><pre>crossent <span class="pl-k">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(
    <span class="pl-v">labels</span><span class="pl-k">=</span>decoder_outputs, <span class="pl-v">logits</span><span class="pl-k">=</span>logits)
train_loss <span class="pl-k">=</span> (tf.reduce_sum(crossent <span class="pl-k">*</span> target_weights) <span class="pl-k">/</span>
    batch_size)</pre></div>
<p>Here, <em>target_weights</em> is a zero-one matrix of the same size as
<em>decoder_outputs</em>. It masks padding positions outside of the target sequence
lengths with values 0.</p>
<p><em><strong>Important note</strong></em>: It's worth pointing out that we divide the loss by
<em>batch_size</em>, so our hyperparameters are "invariant" to batch_size. Some people
divide the loss by (<em>batch_size</em> * <em>num_time_steps</em>), which plays down the
errors made on short sentences. More subtly, our hyperparameters (applied to the
former way) can't be used for the latter way. For example, if both approaches
use SGD with a learning of 1.0, the latter approach effectively uses a much
smaller learning rate of 1 / <em>num_time_steps</em>.</p>
<h3><a id="user-content-gradient-computation--optimization" class="anchor" aria-hidden="true" href="#gradient-computation--optimization"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Gradient computation &amp; optimization</h3>
<p>We have now defined the forward pass of our NMT model. Computing the
backpropagation pass is just a matter of a few lines of code:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Calculate and clip gradients</span>
params <span class="pl-k">=</span> tf.trainable_variables()
gradients <span class="pl-k">=</span> tf.gradients(train_loss, params)
clipped_gradients, _ <span class="pl-k">=</span> tf.clip_by_global_norm(
    gradients, max_gradient_norm)</pre></div>
<p>One of the important steps in training RNNs is gradient clipping. Here, we clip
by the global norm.  The max value, <em>max_gradient_norm</em>, is often set to a value
like 5 or 1. The last step is selecting the optimizer.  The Adam optimizer is a
common choice.  We also select a learning rate.  The value of <em>learning_rate</em>
can is usually in the range 0.0001 to 0.001; and can be set to decrease as
training progresses.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Optimization</span>
optimizer <span class="pl-k">=</span> tf.train.AdamOptimizer(learning_rate)
update_step <span class="pl-k">=</span> optimizer.apply_gradients(
    <span class="pl-c1">zip</span>(clipped_gradients, params))</pre></div>
<p>In our own experiments, we use standard SGD (tf.train.GradientDescentOptimizer)
with a decreasing learning rate schedule, which yields better performance. See
the <a href="#benchmarks">benchmarks</a>.</p>
<h2><a id="user-content-hands-on--lets-train-an-nmt-model" class="anchor" aria-hidden="true" href="#hands-on--lets-train-an-nmt-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hands-on – Let's train an NMT model</h2>
<p>Let's train our very first NMT model, translating from Vietnamese to English!
The entry point of our code
is
<a href="https://github.com/tensorflow/nmt/blob/master/nmt/nmt.py"><strong>nmt.py</strong></a>.</p>
<p>We will use a <em>small-scale parallel corpus of TED talks</em> (133K training
examples) for this exercise. All data we used here can be found
at:
<a href="https://nlp.stanford.edu/projects/nmt/" rel="nofollow">https://nlp.stanford.edu/projects/nmt/</a>. We
will use tst2012 as our dev dataset, and tst2013 as our test dataset.</p>
<p>Run the following command to download the data for training NMT model:<br>
<code>nmt/scripts/download_iwslt15.sh /tmp/nmt_data</code></p>
<p>Run the following command to start the training:</p>
<div class="highlight highlight-source-shell"><pre>mkdir /tmp/nmt_model
python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu</pre></div>
<p>The above command trains a 2-layer LSTM seq2seq model with 128-dim hidden units
and embeddings for 12 epochs. We use a dropout value of 0.2 (keep probability
0.8). If no error, we should see logs similar to the below with decreasing
perplexity values as we train.</p>
<pre><code># First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: &lt;/s&gt; &lt;/s&gt; Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .
    ref: That , of course , was the &lt;unk&gt; distilled from the theories of Karl Marx . &lt;/s&gt; &lt;/s&gt; &lt;/s&gt;
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
</code></pre>
<p>See <a href="https://github.com/tensorflow/nmt/blob/master/nmt/train.py"><strong>train.py</strong></a> for more details.</p>
<p>We can start Tensorboard to view the summary of the model during training:</p>
<div class="highlight highlight-source-shell"><pre>tensorboard --port 22222 --logdir /tmp/nmt_model/</pre></div>
<p>Training the reverse direction from English and Vietnamese can be done simply by changing:<br>
<code>--src=en --tgt=vi</code></p>
<h2><a id="user-content-inference--how-to-generate-translations" class="anchor" aria-hidden="true" href="#inference--how-to-generate-translations"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Inference – How to generate translations</h2>
<p>While you're training your NMT models (and once you have trained models), you
can obtain translations given previously unseen source sentences. This process
is called inference. There is a clear distinction between training and inference
(<em>testing</em>): at inference time, we only have access to the source sentence,
i.e., <em>encoder_inputs</em>. There are many ways to perform decoding.  Decoding
methods include greedy, sampling, and beam-search decoding. Here, we will
discuss the greedy decoding strategy.</p>
<p>The idea is simple and we illustrate it in Figure 3:</p>
<ol>
<li>We still encode the source sentence in the same way as during training to
obtain an <em>encoder_state</em>, and this <em>encoder_state</em> is used to initialize the
decoder.</li>
<li>The decoding (translation) process is started as soon as the decoder receives
a starting symbol "&lt;s&gt;" (refer as <em>tgt_sos_id</em> in our code);</li>
<li>For each timestep on the decoder side, we treat the RNN's output as a set of
logits.  We choose the most likely word, the id associated with the maximum
logit value, as the emitted word (this is the "greedy" behavior).  For
example in Figure 3, the word "moi" has the highest translation probability
in the first decoding step.  We then feed this word as input to the next
timestep.</li>
<li>The process continues until the end-of-sentence marker "&lt;/s&gt;" is produced as
an output symbol (refer as <em>tgt_eos_id</em> in our code).</li>
</ol>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/greedy_dec.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/greedy_dec.jpg" style="max-width:100%;" width="40%"></a>
<br>
Figure 3. <b>Greedy decoding</b> – example of how a trained NMT model produces a
translation for a source sentence "Je suis étudiant" using greedy search.
</p>
<p>Step 3 is what makes inference different from training. Instead of always
feeding the correct target words as an input, inference uses words predicted by
the model. Here's the code to achieve greedy decoding.  It is very similar to
the training decoder.</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Helper</span>
helper <span class="pl-k">=</span> tf.contrib.seq2seq.GreedyEmbeddingHelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

<span class="pl-c"><span class="pl-c">#</span> Decoder</span>
decoder <span class="pl-k">=</span> tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    <span class="pl-v">output_layer</span><span class="pl-k">=</span>projection_layer)
<span class="pl-c"><span class="pl-c">#</span> Dynamic decoding</span>
outputs, _ <span class="pl-k">=</span> tf.contrib.seq2seq.dynamic_decode(
    decoder, <span class="pl-v">maximum_iterations</span><span class="pl-k">=</span>maximum_iterations)
translations <span class="pl-k">=</span> outputs.sample_id</pre></div>
<p>Here, we use <em>GreedyEmbeddingHelper</em> instead of <em>TrainingHelper</em>. Since we do
not know the target sequence lengths in advance, we use <em>maximum_iterations</em> to
limit the translation lengths. One heuristic is to decode up to two times the
source sentence lengths.</p>
<div class="highlight highlight-source-python"><pre>maximum_iterations <span class="pl-k">=</span> tf.round(tf.reduce_max(source_sequence_length) <span class="pl-k">*</span> <span class="pl-c1">2</span>)</pre></div>
<p>Having trained a model, we can now create an inference file and translate some
sentences:</p>
<div class="highlight highlight-source-shell"><pre>cat <span class="pl-k">&gt;</span> /tmp/my_infer_file.vi
<span class="pl-c"><span class="pl-c">#</span> (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)</span>

python -m nmt.nmt \
    --out_dir=/tmp/nmt_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_model/output_infer

cat /tmp/nmt_model/output_infer <span class="pl-c"><span class="pl-c">#</span> To view the inference as output</span></pre></div>
<p>Note the above commands can also be run while the model is still being trained
as long as there exists a training
checkpoint. See <a href="https://github.com/tensorflow/nmt/blob/master/nmt/inference.py"><strong>inference.py</strong></a> for more details.</p>
<h1><a id="user-content-intermediate" class="anchor" aria-hidden="true" href="#intermediate"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Intermediate</h1>
<p>Having gone through the most basic seq2seq model, let's get more advanced! To
build state-of-the-art neural machine translation systems, we will need more
"secret sauce": the <em>attention mechanism</em>, which was first introduced
by <a href="https://arxiv.org/abs/1409.0473" rel="nofollow">Bahdanau et al., 2015</a>, then later refined
by <a href="https://arxiv.org/abs/1508.04025" rel="nofollow">Luong et al., 2015</a> and others. The key
idea of the attention mechanism is to establish direct short-cut connections
between the target and the source by paying "attention" to relevant source
content as we translate. A nice byproduct of the attention mechanism is an
easy-to-visualize alignment matrix between the source and target sentences (as
shown in Figure 4).</p>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/attention_vis.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/attention_vis.jpg" style="max-width:100%;" width="40%"></a>
<br>
Figure 4. <b>Attention visualization</b> – example of the alignments between source
and target sentences. Image is taken from (Bahdanau et al., 2015).
</p>
<p>Remember that in the vanilla seq2seq model, we pass the last source state from
the encoder to the decoder when starting the decoding process. This works well
for short and medium-length sentences; however, for long sentences, the single
fixed-size hidden state becomes an information bottleneck. Instead of discarding
all of the hidden states computed in the source RNN, the attention mechanism
provides an approach that allows the decoder to peek at them (treating them as a
dynamic memory of the source information). By doing so, the attention mechanism
improves the translation of longer sentences. Nowadays, attention mechanisms are
the defacto standard and have been successfully applied to many other tasks
(including image caption generation, speech recognition, and text
summarization).</p>
<h2><a id="user-content-background-on-the-attention-mechanism" class="anchor" aria-hidden="true" href="#background-on-the-attention-mechanism"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Background on the Attention Mechanism</h2>
<p>We now describe an instance of the attention mechanism proposed in (Luong et
al., 2015), which has been used in several state-of-the-art systems including
open-source toolkits such as <a href="http://opennmt.net/about/" rel="nofollow">OpenNMT</a> and in the TF
seq2seq API in this tutorial. We will also provide connections to other variants
of the attention mechanism.</p>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/attention_mechanism.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/attention_mechanism.jpg" style="max-width:100%;" width="48%"></a>
<br>
Figure 5. <b>Attention mechanism</b> – example of an attention-based NMT system
as described in (Luong et al., 2015) . We highlight in detail the first step of
the attention computation. For clarity, we don't show the embedding and
projection layers in Figure (2).
</p>
<p>As illustrated in Figure 5, the attention computation happens at every decoder
time step.  It consists of the following stages:</p>
<ol>
<li>The current target hidden state is compared with all source states to derive
<em>attention weights</em> (can be visualized as in Figure 4).</li>
<li>Based on the attention weights we compute a <em>context vector</em> as the weighted
average of the source states.</li>
<li>Combine the context vector with the current target hidden state to yield the
final <em>attention vector</em></li>
<li>The attention vector is fed as an input to the next time step (<em>input
feeding</em>).  The first three steps can be summarized by the equations below:</li>
</ol>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/attention_equation_0.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/attention_equation_0.jpg" style="max-width:100%;" width="80%"></a>
<br>
</p>
<p>Here, the function <code>score</code> is used to compared the target hidden state $$h_t$$
with each of the source hidden states $$\overline{h}_s$$, and the result is normalized to
produced attention weights (a distribution over source positions). There are
various choices of the scoring function; popular scoring functions include the
multiplicative and additive forms given in Eq. (4). Once computed, the attention
vector $$a_t$$ is used to derive the softmax logit and loss.  This is similar to the
target hidden state at the top layer of a vanilla seq2seq model. The function
<code>f</code> can also take other forms.</p>
<p align="center">
<a target="_blank" href="https://github.com/tensorflow/nmt/blob/master/nmt/g3doc/img/attention_equation_1.jpg"><img src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/attention_equation_1.jpg" style="max-width:100%;" width="80%"></a>
<br>
</p>
<p>Various implementations of attention mechanisms can be found
in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py">attention_wrapper.py</a>.</p>
<p><em><strong>What matters in the attention mechanism?</strong></em></p>
<p>As hinted in the above equations, there are many different attention variants.
These variants depend on the form of the scoring function and the attention
function, and on whether the previous state $$h_{t-1}$$ is used instead of
$$h_t$$ in the scoring function as originally suggested in (Bahdanau et al.,
2015). Empirically, we found that only certain choices matter. First, the basic
form of attention, i.e., direct connections between target and source, needs to
be present. Second, it's important to feed the attention vector to the next
timestep to inform the network about past attention decisions as demonstrated in
(Luong et al., 2015). Lastly, choices of the scoring function can often result
in different performance. See more in the <a href="#benchmarks">benchmark results</a>
section.</p>
<h2><a id="user-content-attention-wrapper-api" class="anchor" aria-hidden="true" href="#attention-wrapper-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Attention Wrapper API</h2>
<p>In our implementation of
the
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py">AttentionWrapper</a>,
we borrow some terminology
from <a href="https://arxiv.org/abs/1410.3916" rel="nofollow">(Weston et al., 2015)</a> in their work on
<em>memory networks</em>. Instead of having readable &amp; writable memory, the attention
mechanism presented in this tutorial is a <em>read-only</em> memory. Specifically, the
set of source hidden states (or their transformed versions, e.g.,
$$W\overline{h}_s$$ in Luong's scoring style or $$W_2\overline{h}_s$$ in
Bahdanau's scoring style) is referred to as the <em>"memory"</em>. At each time step,
we use the current target hidden state as a <em>"query"</em> to decide on which parts
of the memory to read.  Usually, the query needs to be compared with keys
corresponding to individual memory slots. In the above presentation of the
attention mechanism, we happen to use the set of source hidden states (or their
transformed versions, e.g., $$W_1h_t$$ in Bahdanau's scoring style) as
"keys". One can be inspired by this memory-network terminology to derive other
forms of attention!</p>
<p>Thanks to the attention wrapper, extending our vanilla seq2seq code with
attention is trivial. This part refers to
file <a href="https://github.com/tensorflow/nmt/blob/master/nmt/attention_model.py"><strong>attention_model.py</strong></a></p>
<p>First, we need to define an attention mechanism, e.g., from (Luong et al.,
2015):</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> attention_states: [batch_size, max_time, num_units]</span>
attention_states <span class="pl-k">=</span> tf.transpose(encoder_outputs, [<span class="pl-c1">1</span>, <span class="pl-c1">0</span>, <span class="pl-c1">2</span>])

<span class="pl-c"><span class="pl-c">#</span> Create an attention mechanism</span>
attention_mechanism <span class="pl-k">=</span> tf.contrib.seq2seq.LuongAttention(
    num_units, attention_states,
    <span class="pl-v">memory_sequence_length</span><span class="pl-k">=</span>source_sequence_length)</pre></div>
<p>In the previous <a href="#encoder">Encoder</a> section, <em>encoder_outputs</em> is the set of all
source hidden states at the top layer and has the shape of <em>[max_time,
batch_size, num_units]</em> (since we use <em>dynamic_rnn</em> with <em>time_major</em> set to
<em>True</em> for efficiency). For the attention mechanism, we need to make sure the
"memory" passed in is batch major, so we need to transpose
<em>attention_states</em>. We pass <em>source_sequence_length</em> to the attention mechanism
to ensure that the attention weights are properly normalized (over non-padding
positions only).</p>
<p>Having defined an attention mechanism, we use <em>AttentionWrapper</em> to wrap the
decoding cell:</p>
<div class="highlight highlight-source-python"><pre>decoder_cell <span class="pl-k">=</span> tf.contrib.seq2seq.AttentionWrapper(
    decoder_cell, attention_mechanism,
    <span class="pl-v">attention_layer_size</span><span class="pl-k">=</span>num_units)</pre></div>
<p>The rest of the code is almost the same as in the Section <a href="#decoder">Decoder</a>!</p>
<h2><a id="user-content-hands-on--building-an-attention-based-nmt-model" class="anchor" aria-hidden="true" href="#hands-on--building-an-attention-based-nmt-model"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hands-on – building an attention-based NMT model</h2>
<p>To enable attention, we need to use one of <code>luong</code>, <code>scaled_luong</code>, <code>bahdanau</code>
or <code>normed_bahdanau</code> as the value of the <code>attention</code> flag during training. The
flag specifies which attention mechanism we are going to use. In addition, we
need to create a new directory for the attention model, so we don't reuse the
previously trained basic NMT model.</p>
<p>Run the following command to start the training:</p>
<div class="highlight highlight-source-shell"><pre>mkdir /tmp/nmt_attention_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu</pre></div>
<p>After training, we can use the same inference command with the new out_dir for
inference:</p>
<div class="highlight highlight-source-shell"><pre>python -m nmt.nmt \
    --out_dir=/tmp/nmt_attention_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_attention_model/output_infer</pre></div>
<h1><a id="user-content-tips--tricks" class="anchor" aria-hidden="true" href="#tips--tricks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tips &amp; Tricks</h1>
<h2><a id="user-content-building-training-eval-and-inference-graphs" class="anchor" aria-hidden="true" href="#building-training-eval-and-inference-graphs"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Building Training, Eval, and Inference Graphs</h2>
<p>When building a machine learning model in TensorFlow, it's often best to build
three separate graphs:</p>
<ul>
<li>
<p>The Training graph, which:</p>
<ul>
<li>Batches, buckets, and possibly subsamples input data from a set of
files/external inputs.</li>
<li>Includes the forward and backprop ops.</li>
<li>Constructs the optimizer, and adds the training op.</li>
</ul>
</li>
<li>
<p>The Eval graph, which:</p>
<ul>
<li>Batches and buckets input data from a set of files/external inputs.</li>
<li>Includes the training forward ops, and additional evaluation ops that
aren't used for training.</li>
</ul>
</li>
<li>
<p>The Inference graph, which:</p>
<ul>
<li>May not batch input data.</li>
<li>Does not subsample or bucket input data.</li>
<li>Reads input data from placeholders (data can be fed directly to the graph
via <em>feed_dict</em> or from a C++ TensorFlow serving binary).</li>
<li>Includes a subset of the model forward ops, and possibly additional
special inputs/outputs for storing state between session.run calls.</li>
</ul>
</li>
</ul>
<p>Building separate graphs has several benefits:</p>
<ul>
<li>The inference graph is usually very different from the other two, so it makes
sense to build it separately.</li>
<li>The eval graph becomes simpler since it no longer has all the additional
backprop ops.</li>
<li>Data feeding can be implemented separately for each graph.</li>
<li>Variable reuse is much simpler.  For example, in the eval graph there's no
need to reopen variable scopes with <em>reuse=True</em> just because the Training
model created these variables already.  So the same code can be reused
without sprinkling <em>reuse=</em> arguments everywhere.</li>
<li>In distributed training, it is commonplace to have separate workers perform
training, eval, and inference.  These need to build their own graphs anyway.
So building the system this way prepares you for distributed training.</li>
</ul>
<p>The primary source of complexity becomes how to share Variables across the three
graphs in a single machine setting. This is solved by using a separate session
for each graph. The training session periodically saves checkpoints, and the
eval session and the infer session restore parameters from checkpoints. The
example below shows the main differences between the two approaches.</p>
<p><strong>Before: Three models in a single graph and sharing a single Session</strong></p>
<div class="highlight highlight-source-python"><pre><span class="pl-k">with</span> tf.variable_scope(<span class="pl-s"><span class="pl-pds">'</span>root<span class="pl-pds">'</span></span>):
  train_inputs <span class="pl-k">=</span> tf.placeholder()
  train_op, loss <span class="pl-k">=</span> BuildTrainModel(train_inputs)
  initializer <span class="pl-k">=</span> tf.global_variables_initializer()

<span class="pl-k">with</span> tf.variable_scope(<span class="pl-s"><span class="pl-pds">'</span>root<span class="pl-pds">'</span></span>, <span class="pl-v">reuse</span><span class="pl-k">=</span><span class="pl-c1">True</span>):
  eval_inputs <span class="pl-k">=</span> tf.placeholder()
  eval_loss <span class="pl-k">=</span> BuildEvalModel(eval_inputs)

<span class="pl-k">with</span> tf.variable_scope(<span class="pl-s"><span class="pl-pds">'</span>root<span class="pl-pds">'</span></span>, <span class="pl-v">reuse</span><span class="pl-k">=</span><span class="pl-c1">True</span>):
  infer_inputs <span class="pl-k">=</span> tf.placeholder()
  inference_output <span class="pl-k">=</span> BuildInferenceModel(infer_inputs)

sess <span class="pl-k">=</span> tf.Session()

sess.run(initializer)

<span class="pl-k">for</span> i <span class="pl-k">in</span> itertools.count():
  train_input_data <span class="pl-k">=</span> <span class="pl-c1">...</span>
  sess.run([loss, train_op], <span class="pl-v">feed_dict</span><span class="pl-k">=</span>{train_inputs: train_input_data})

  <span class="pl-k">if</span> i <span class="pl-k">%</span> <span class="pl-c1">EVAL_STEPS</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
    <span class="pl-k">while</span> data_to_eval:
      eval_input_data <span class="pl-k">=</span> <span class="pl-c1">...</span>
      sess.run([eval_loss], <span class="pl-v">feed_dict</span><span class="pl-k">=</span>{eval_inputs: eval_input_data})

  <span class="pl-k">if</span> i <span class="pl-k">%</span> <span class="pl-c1">INFER_STEPS</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
    sess.run(inference_output, <span class="pl-v">feed_dict</span><span class="pl-k">=</span>{infer_inputs: infer_input_data})</pre></div>
<p><strong>After: Three models in three graphs, with three Sessions sharing the same Variables</strong></p>
<div class="highlight highlight-source-python"><pre>train_graph <span class="pl-k">=</span> tf.Graph()
eval_graph <span class="pl-k">=</span> tf.Graph()
infer_graph <span class="pl-k">=</span> tf.Graph()

<span class="pl-k">with</span> train_graph.as_default():
  train_iterator <span class="pl-k">=</span> <span class="pl-c1">...</span>
  train_model <span class="pl-k">=</span> BuildTrainModel(train_iterator)
  initializer <span class="pl-k">=</span> tf.global_variables_initializer()

<span class="pl-k">with</span> eval_graph.as_default():
  eval_iterator <span class="pl-k">=</span> <span class="pl-c1">...</span>
  eval_model <span class="pl-k">=</span> BuildEvalModel(eval_iterator)

<span class="pl-k">with</span> infer_graph.as_default():
  infer_iterator, infer_inputs <span class="pl-k">=</span> <span class="pl-c1">...</span>
  infer_model <span class="pl-k">=</span> BuildInferenceModel(infer_iterator)

checkpoints_path <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>/tmp/model/checkpoints<span class="pl-pds">"</span></span>

train_sess <span class="pl-k">=</span> tf.Session(<span class="pl-v">graph</span><span class="pl-k">=</span>train_graph)
eval_sess <span class="pl-k">=</span> tf.Session(<span class="pl-v">graph</span><span class="pl-k">=</span>eval_graph)
infer_sess <span class="pl-k">=</span> tf.Session(<span class="pl-v">graph</span><span class="pl-k">=</span>infer_graph)

train_sess.run(initializer)
train_sess.run(train_iterator.initializer)

<span class="pl-k">for</span> i <span class="pl-k">in</span> itertools.count():

  train_model.train(train_sess)

  <span class="pl-k">if</span> i <span class="pl-k">%</span> <span class="pl-c1">EVAL_STEPS</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
    checkpoint_path <span class="pl-k">=</span> train_model.saver.save(train_sess, checkpoints_path, <span class="pl-v">global_step</span><span class="pl-k">=</span>i)
    eval_model.saver.restore(eval_sess, checkpoint_path)
    eval_sess.run(eval_iterator.initializer)
    <span class="pl-k">while</span> data_to_eval:
      eval_model.eval(eval_sess)

  <span class="pl-k">if</span> i <span class="pl-k">%</span> <span class="pl-c1">INFER_STEPS</span> <span class="pl-k">==</span> <span class="pl-c1">0</span>:
    checkpoint_path <span class="pl-k">=</span> train_model.saver.save(train_sess, checkpoints_path, <span class="pl-v">global_step</span><span class="pl-k">=</span>i)
    infer_model.saver.restore(infer_sess, checkpoint_path)
    infer_sess.run(infer_iterator.initializer, <span class="pl-v">feed_dict</span><span class="pl-k">=</span>{infer_inputs: infer_input_data})
    <span class="pl-k">while</span> data_to_infer:
      infer_model.infer(infer_sess)</pre></div>
<p>Notice how the latter approach is "ready" to be converted to a distributed
version.</p>
<p>One other difference in the new approach is that instead of using <em>feed_dicts</em>
to feed data at each <em>session.run</em> call (and thereby performing our own
batching, bucketing, and manipulating of data), we use stateful iterator
objects.  These iterators make the input pipeline much easier in both the
single-machine and distributed setting. We will cover the new input data
pipeline (as introduced in TensorFlow 1.2) in the next section.</p>
<h2><a id="user-content-data-input-pipeline" class="anchor" aria-hidden="true" href="#data-input-pipeline"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Input Pipeline</h2>
<p>Prior to TensorFlow 1.2, users had two options for feeding data to the
TensorFlow training and eval pipelines:</p>
<ol>
<li>Feed data directly via <em>feed_dict</em> at each training <em>session.run</em> call.</li>
<li>Use the queueing mechanisms in <em>tf.train</em> (e.g. <em>tf.train.batch</em>) and
<em>tf.contrib.train</em>.</li>
<li>Use helpers from a higher level framework like <em>tf.contrib.learn</em> or
<em>tf.contrib.slim</em> (which effectively use #2).</li>
</ol>
<p>The first approach is easier for users who aren't familiar with TensorFlow or
need to do exotic input modification (i.e., their own minibatch queueing) that
can only be done in Python.  The second and third approaches are more standard
but a little less flexible; they also require starting multiple python threads
(queue runners).  Furthermore, if used incorrectly queues can lead to deadlocks
or opaque error messages.  Nevertheless, queues are significantly more efficient
than using <em>feed_dict</em> and are the standard for both single-machine and
distributed training.</p>
<p>Starting in TensorFlow 1.2, there is a new system available for reading data
into TensorFlow models: dataset iterators, as found in the <strong>tf.data</strong>
module. Data iterators are flexible, easy to reason about and to manipulate, and
provide efficiency and multithreading by leveraging the TensorFlow C++ runtime.</p>
<p>A <strong>dataset</strong> can be created from a batch data Tensor, a filename, or a Tensor
containing multiple filenames.  Some examples:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Training dataset consists of multiple files.</span>
train_dataset <span class="pl-k">=</span> tf.data.TextLineDataset(train_files)

<span class="pl-c"><span class="pl-c">#</span> Evaluation dataset uses a single file, but we may</span>
<span class="pl-c"><span class="pl-c">#</span> point to a different file for each evaluation round.</span>
eval_file <span class="pl-k">=</span> tf.placeholder(tf.string, <span class="pl-v">shape</span><span class="pl-k">=</span>())
eval_dataset <span class="pl-k">=</span> tf.data.TextLineDataset(eval_file)

<span class="pl-c"><span class="pl-c">#</span> For inference, feed input data to the dataset directly via feed_dict.</span>
infer_batch <span class="pl-k">=</span> tf.placeholder(tf.string, <span class="pl-v">shape</span><span class="pl-k">=</span>(num_infer_examples,))
infer_dataset <span class="pl-k">=</span> tf.data.Dataset.from_tensor_slices(infer_batch)</pre></div>
<p>All datasets can be treated similarly via input processing.  This includes
reading and cleaning the data, bucketing (in the case of training and eval),
filtering, and batching.</p>
<p>To convert each sentence into vectors of word strings, for example, we use the
dataset map transformation:</p>
<div class="highlight highlight-source-python"><pre>dataset <span class="pl-k">=</span> dataset.map(<span class="pl-k">lambda</span> <span class="pl-smi">string</span>: tf.string_split([string]).values)</pre></div>
<p>We can then switch each sentence vector into a tuple containing both the vector
and its dynamic length:</p>
<div class="highlight highlight-source-python"><pre>dataset <span class="pl-k">=</span> dataset.map(<span class="pl-k">lambda</span> <span class="pl-smi">words</span>: (words, tf.size(words))</pre></div>
<p>Finally, we can perform a vocabulary lookup on each sentence.  Given a lookup
table object table, this map converts the first tuple elements from a vector of
strings to a vector of integers.</p>
<div class="highlight highlight-source-python"><pre>dataset <span class="pl-k">=</span> dataset.map(<span class="pl-k">lambda</span> <span class="pl-smi">words</span>, <span class="pl-smi">size</span>: (table.lookup(words), size))</pre></div>
<p>Joining two datasets is also easy.  If two files contain line-by-line
translations of each other and each one is read into its own dataset, then a new
dataset containing the tuples of the zipped lines can be created via:</p>
<div class="highlight highlight-source-python"><pre>source_target_dataset <span class="pl-k">=</span> tf.data.Dataset.zip((source_dataset, target_dataset))</pre></div>
<p>Batching of variable-length sentences is straightforward. The following
transformation batches <em>batch_size</em> elements from <em>source_target_dataset</em>, and
respectively pads the source and target vectors to the length of the longest
source and target vector in each batch.</p>
<div class="highlight highlight-source-python"><pre>batched_dataset <span class="pl-k">=</span> source_target_dataset.padded_batch(
        batch_size,
        <span class="pl-v">padded_shapes</span><span class="pl-k">=</span>((tf.TensorShape([<span class="pl-c1">None</span>]),  <span class="pl-c"><span class="pl-c">#</span> source vectors of unknown size</span>
                        tf.TensorShape([])),     <span class="pl-c"><span class="pl-c">#</span> size(source)</span>
                       (tf.TensorShape([<span class="pl-c1">None</span>]),  <span class="pl-c"><span class="pl-c">#</span> target vectors of unknown size</span>
                        tf.TensorShape([]))),    <span class="pl-c"><span class="pl-c">#</span> size(target)</span>
        <span class="pl-v">padding_values</span><span class="pl-k">=</span>((src_eos_id,  <span class="pl-c"><span class="pl-c">#</span> source vectors padded on the right with src_eos_id</span>
                         <span class="pl-c1">0</span>),          <span class="pl-c"><span class="pl-c">#</span> size(source) -- unused</span>
                        (tgt_eos_id,  <span class="pl-c"><span class="pl-c">#</span> target vectors padded on the right with tgt_eos_id</span>
                         <span class="pl-c1">0</span>)))         <span class="pl-c"><span class="pl-c">#</span> size(target) -- unused</span></pre></div>
<p>Values emitted from this dataset will be nested tuples whose tensors have a
leftmost dimension of size <em>batch_size</em>.  The structure will be:</p>
<ul>
<li>iterator[0][0] has the batched and padded source sentence matrices.</li>
<li>iterator[0][1] has the batched source size vectors.</li>
<li>iterator[1][0] has the batched and padded target sentence matrices.</li>
<li>iterator[1][1] has the batched target size vectors.</li>
</ul>
<p>Finally, bucketing that batches similarly-sized source sentences together is
also possible.  Please see the
file
<a href="https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py">utils/iterator_utils.py</a> for
more details and the full implementation.</p>
<p>Reading data from a Dataset requires three lines of code: create the iterator,
get its values, and initialize it.</p>
<div class="highlight highlight-source-python"><pre>batched_iterator <span class="pl-k">=</span> batched_dataset.make_initializable_iterator()

((source, source_lengths), (target, target_lengths)) <span class="pl-k">=</span> batched_iterator.get_next()

<span class="pl-c"><span class="pl-c">#</span> At initialization time.</span>
session.run(batched_iterator.initializer, <span class="pl-v">feed_dict</span><span class="pl-k">=</span>{<span class="pl-c1">...</span>})</pre></div>
<p>Once the iterator is initialized, every <em>session.run</em> call that accesses source
or target tensors will request the next minibatch from the underlying dataset.</p>
<h2><a id="user-content-other-details-for-better-nmt-models" class="anchor" aria-hidden="true" href="#other-details-for-better-nmt-models"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Other details for better NMT models</h2>
<h3><a id="user-content-bidirectional-rnns" class="anchor" aria-hidden="true" href="#bidirectional-rnns"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Bidirectional RNNs</h3>
<p>Bidirectionality on the encoder side generally gives better performance (with
some degradation in speed as more layers are used). Here, we give a simplified
example of how to build an encoder with a single bidirectional layer:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Construct forward and backward cells</span>
forward_cell <span class="pl-k">=</span> tf.nn.rnn_cell.BasicLSTMCell(num_units)
backward_cell <span class="pl-k">=</span> tf.nn.rnn_cell.BasicLSTMCell(num_units)

bi_outputs, encoder_state <span class="pl-k">=</span> tf.nn.bidirectional_dynamic_rnn(
    forward_cell, backward_cell, encoder_emb_inp,
    <span class="pl-v">sequence_length</span><span class="pl-k">=</span>source_sequence_length, <span class="pl-v">time_major</span><span class="pl-k">=</span><span class="pl-c1">True</span>)
encoder_outputs <span class="pl-k">=</span> tf.concat(bi_outputs, <span class="pl-k">-</span><span class="pl-c1">1</span>)</pre></div>
<p>The variables <em>encoder_outputs</em> and <em>encoder_state</em> can be used in the same way
as in Section Encoder. Note that, for multiple bidirectional layers, we need to
manipulate the encoder_state a bit, see <a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py">model.py</a>, method
<em>_build_bidirectional_rnn()</em> for more details.</p>
<h3><a id="user-content-beam-search" class="anchor" aria-hidden="true" href="#beam-search"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Beam search</h3>
<p>While greedy decoding can give us quite reasonable translation quality, a beam
search decoder can further boost performance. The idea of beam search is to
better explore the search space of all possible translations by keeping around a
small set of top candidates as we translate. The size of the beam is called
<em>beam width</em>; a minimal beam width of, say size 10, is generally sufficient. For
more information, we refer readers to Section 7.2.3
of <a href="https://arxiv.org/abs/1703.01619" rel="nofollow">Neubig, (2017)</a>. Here's an example of how
beam search can be done:</p>
<div class="highlight highlight-source-python"><pre><span class="pl-c"><span class="pl-c">#</span> Replicate encoder infos beam_width times</span>
decoder_initial_state <span class="pl-k">=</span> tf.contrib.seq2seq.tile_batch(
    encoder_state, <span class="pl-v">multiplier</span><span class="pl-k">=</span>hparams.beam_width)

<span class="pl-c"><span class="pl-c">#</span> Define a beam-search decoder</span>
decoder <span class="pl-k">=</span> tf.contrib.seq2seq.BeamSearchDecoder(
        <span class="pl-v">cell</span><span class="pl-k">=</span>decoder_cell,
        <span class="pl-v">embedding</span><span class="pl-k">=</span>embedding_decoder,
        <span class="pl-v">start_tokens</span><span class="pl-k">=</span>start_tokens,
        <span class="pl-v">end_token</span><span class="pl-k">=</span>end_token,
        <span class="pl-v">initial_state</span><span class="pl-k">=</span>decoder_initial_state,
        <span class="pl-v">beam_width</span><span class="pl-k">=</span>beam_width,
        <span class="pl-v">output_layer</span><span class="pl-k">=</span>projection_layer,
        <span class="pl-v">length_penalty_weight</span><span class="pl-k">=</span><span class="pl-c1">0.0</span>)

<span class="pl-c"><span class="pl-c">#</span> Dynamic decoding</span>
outputs, _ <span class="pl-k">=</span> tf.contrib.seq2seq.dynamic_decode(decoder, <span class="pl-c1">...</span>)</pre></div>
<p>Note that the same <em>dynamic_decode()</em> API call is used, similar to the
Section <a href="#decoder">Decoder</a>. Once decoded, we can access the translations as
follows:</p>
<div class="highlight highlight-source-python"><pre>translations <span class="pl-k">=</span> outputs.predicted_ids
<span class="pl-c"><span class="pl-c">#</span> Make sure translations shape is [batch_size, beam_width, time]</span>
<span class="pl-k">if</span> <span class="pl-c1">self</span>.time_major:
   translations <span class="pl-k">=</span> tf.transpose(translations, <span class="pl-v">perm</span><span class="pl-k">=</span>[<span class="pl-c1">1</span>, <span class="pl-c1">2</span>, <span class="pl-c1">0</span>])</pre></div>
<p>See <a href="https://github.com/tensorflow/nmt/blob/master/nmt/model.py">model.py</a>, method <em>_build_decoder()</em> for more details.</p>
<h3><a id="user-content-hyperparameters" class="anchor" aria-hidden="true" href="#hyperparameters"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hyperparameters</h3>
<p>There are several hyperparameters that can lead to additional
performances. Here, we list some based on our own experience [ Disclaimers:
others might not agree on things we wrote! ].</p>
<p><em><strong>Optimizer</strong></em>: while Adam can lead to reasonable results for "unfamiliar"
architectures, SGD with scheduling will generally lead to better performance if
you can train with SGD.</p>
<p><em><strong>Attention</strong></em>: Bahdanau-style attention often requires bidirectionality on the
encoder side to work well; whereas Luong-style attention tends to work well for
different settings. For this tutorial code, we recommend using the two improved
variants of Luong &amp; Bahdanau-style attentions: <em>scaled_luong</em> &amp; <em>normed
bahdanau</em>.</p>
<h3><a id="user-content-multi-gpu-training" class="anchor" aria-hidden="true" href="#multi-gpu-training"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Multi-GPU training</h3>
<p>Training a NMT model may take several days. Placing different RNN layers on
different GPUs can improve the training speed. Here’s an example to create
RNN layers on multiple GPUs.</p>
<div class="highlight highlight-source-python"><pre>cells <span class="pl-k">=</span> []
<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">range</span>(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      <span class="pl-s"><span class="pl-pds">"</span>/gpu:<span class="pl-c1">%d</span><span class="pl-pds">"</span></span> <span class="pl-k">%</span> (num_layers <span class="pl-k">%</span> num_gpus)))
cell <span class="pl-k">=</span> tf.contrib.rnn.MultiRNNCell(cells)</pre></div>
<p>In addition, we need to enable the <code>colocate_gradients_with_ops</code> option in
<code>tf.gradients</code> to parallelize the gradients computation.</p>
<p>You may notice the speed improvement of the attention based NMT model is very
small as the number of GPUs increases. One major drawback of the standard
attention architecture is using the top (final) layer’s output to query
attention at each time step. That means each decoding step must wait its
previous step completely finished; hence, we can’t parallelize the decoding
process by simply placing RNN layers on multiple GPUs.</p>
<p>The <a href="https://arxiv.org/pdf/1609.08144.pdf" rel="nofollow">GNMT attention architecture</a>
parallelizes the decoder's computation by using the bottom (first) layer’s
output to query attention. Therefore, each decoding step can start as soon as
its previous step's first layer and attention computation finished. We
implemented the architecture in
<a href="https://github.com/tensorflow/nmt/blob/master/nmt/gnmt_model.py">GNMTAttentionMultiCell</a>,
a subclass of <em>tf.contrib.rnn.MultiRNNCell</em>. Here’s an example of how to create
a decoder cell with the <em>GNMTAttentionMultiCell</em>.</p>
<div class="highlight highlight-source-python"><pre>cells <span class="pl-k">=</span> []
<span class="pl-k">for</span> i <span class="pl-k">in</span> <span class="pl-c1">range</span>(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      <span class="pl-s"><span class="pl-pds">"</span>/gpu:<span class="pl-c1">%d</span><span class="pl-pds">"</span></span> <span class="pl-k">%</span> (num_layers <span class="pl-k">%</span> num_gpus)))
attention_cell <span class="pl-k">=</span> cells.pop(<span class="pl-c1">0</span>)
attention_cell <span class="pl-k">=</span> tf.contrib.seq2seq.AttentionWrapper(
    attention_cell,
    attention_mechanism,
    <span class="pl-v">attention_layer_size</span><span class="pl-k">=</span><span class="pl-c1">None</span>,  <span class="pl-c"><span class="pl-c">#</span> don't add an additional dense layer.</span>
    <span class="pl-v">output_attention</span><span class="pl-k">=</span><span class="pl-c1">False</span>,)
cell <span class="pl-k">=</span> GNMTAttentionMultiCell(attention_cell, cells)</pre></div>
<h1><a id="user-content-benchmarks" class="anchor" aria-hidden="true" href="#benchmarks"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Benchmarks</h1>
<h2><a id="user-content-iwslt-english-vietnamese" class="anchor" aria-hidden="true" href="#iwslt-english-vietnamese"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>IWSLT English-Vietnamese</h2>
<p>Train: 133K examples, vocab=vocab.(vi|en), train=train.(vi|en)
dev=tst2012.(vi|en),
test=tst2013.(vi|en), <a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/download_iwslt15.sh">download script</a>.</p>
<p><em><strong>Training details</strong></em>. We train 2-layer LSTMs of 512 units with bidirectional
encoder (i.e., 1 bidirectional layers for the encoder), embedding dim
is 512. LuongAttention (scale=True) is used together with dropout keep_prob of
0.8. All parameters are uniformly. We use SGD with learning rate 1.0 as follows:
train for 12K steps (~ 12 epochs); after 8K steps, we start halving learning
rate every 1K step.</p>
<p><em><strong>Results</strong></em>.</p>
<p>Below are the averaged results of 2 models
(<a href="http://download.tensorflow.org/models/nmt/envi_model_1.zip" rel="nofollow">model 1</a>,
<a href="http://download.tensorflow.org/models/nmt/envi_model_2.zip" rel="nofollow">model 2</a>).<br>
We measure the translation quality in terms of BLEU scores <a href="http://www.aclweb.org/anthology/P02-1040.pdf" rel="nofollow">(Papineni et al., 2002)</a>.</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th align="center">tst2012 (dev)</th>
<th align="center">test2013 (test)</th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (greedy)</td>
<td align="center">23.2</td>
<td align="center">25.5</td>
</tr>
<tr>
<td>NMT (beam=10)</td>
<td align="center">23.8</td>
<td align="center"><strong>26.1</strong></td>
</tr>
<tr>
<td><a href="https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf" rel="nofollow">(Luong &amp; Manning, 2015)</a></td>
<td align="center">-</td>
<td align="center">23.3</td>
</tr></tbody></table>
<p><strong>Training Speed</strong>: (0.37s step-time, 15.3K wps) on <em>K40m</em> &amp; (0.17s step-time, 32.2K wps) on <em>TitanX</em>.<br>
Here, step-time means the time taken to run one mini-batch (of size 128). For wps, we count words on both the source and target.</p>
<h2><a id="user-content-wmt-german-english" class="anchor" aria-hidden="true" href="#wmt-german-english"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>WMT German-English</h2>
<p>Train: 4.5M examples, vocab=vocab.bpe.32000.(de|en),
train=train.tok.clean.bpe.32000.(de|en), dev=newstest2013.tok.bpe.32000.(de|en),
test=newstest2015.tok.bpe.32000.(de|en),
<a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/wmt16_en_de.sh">download script</a></p>
<p><em><strong>Training details</strong></em>. Our training hyperparameters are similar to the
English-Vietnamese experiments except for the following details. The data is
split into subword units using <a href="https://github.com/rsennrich/subword-nmt">BPE</a>
(32K operations). We train 4-layer LSTMs of 1024 units with bidirectional
encoder (i.e., 2 bidirectional layers for the encoder), embedding dim
is 1024. We train for 350K steps (~ 10 epochs); after 170K steps, we start
halving learning rate every 17K step.</p>
<p><em><strong>Results</strong></em>.</p>
<p>The first 2 rows are the averaged results of 2 models
(<a href="http://download.tensorflow.org/models/nmt/deen_model_1.zip" rel="nofollow">model 1</a>,
<a href="http://download.tensorflow.org/models/nmt/deen_model_2.zip" rel="nofollow">model 2</a>).
Results in the third row is with GNMT attention
(<a href="http://download.tensorflow.org/models/nmt/10122017/deen_gnmt_model_4_layer.zip" rel="nofollow">model</a>)
; trained with 4 GPUs.</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th align="center">newstest2013 (dev)</th>
<th align="center">newstest2015</th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (greedy)</td>
<td align="center">27.1</td>
<td align="center">27.6</td>
</tr>
<tr>
<td>NMT (beam=10)</td>
<td align="center">28.0</td>
<td align="center">28.9</td>
</tr>
<tr>
<td>NMT + GNMT attention (beam=10)</td>
<td align="center">29.0</td>
<td align="center"><strong>29.9</strong></td>
</tr>
<tr>
<td><a href="http://matrix.statmt.org/" rel="nofollow">WMT SOTA</a></td>
<td align="center">-</td>
<td align="center">29.3</td>
</tr></tbody></table>
<p>These results show that our code builds strong baseline systems for NMT.<br>
(Note that WMT systems generally utilize a huge amount monolingual data which we currently do not.)</p>
<p><strong>Training Speed</strong>: (2.1s step-time, 3.4K wps) on <em>Nvidia K40m</em> &amp; (0.7s step-time, 8.7K wps) on <em>Nvidia TitanX</em> for standard models.<br>
To see the speed-ups with GNMT attention, we benchmark on <em>K40m</em> only:</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th align="center">1 gpu</th>
<th align="center">4 gpus</th>
<th align="center">8 gpus</th>
</tr>
</thead>
<tbody>
<tr>
<td>NMT (4 layers)</td>
<td align="center">2.2s, 3.4K</td>
<td align="center">1.9s, 3.9K</td>
<td align="center">-</td>
</tr>
<tr>
<td>NMT (8 layers)</td>
<td align="center">3.5s, 2.0K</td>
<td align="center">-</td>
<td align="center">2.9s, 2.4K</td>
</tr>
<tr>
<td>NMT + GNMT attention (4 layers)</td>
<td align="center">2.6s, 2.8K</td>
<td align="center">1.7s, 4.3K</td>
<td align="center">-</td>
</tr>
<tr>
<td>NMT + GNMT attention (8 layers)</td>
<td align="center">4.2s, 1.7K</td>
<td align="center">-</td>
<td align="center">1.9s, 3.8K</td>
</tr></tbody></table>
<p>These results show that without GNMT attention, the gains from using multiple gpus are minimal.<br>
With GNMT attention, we obtain from 50%-100% speed-ups with multiple gpus.</p>
<h2><a id="user-content-wmt-english-german--full-comparison" class="anchor" aria-hidden="true" href="#wmt-english-german--full-comparison"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>WMT English-German — Full Comparison</h2>
<p>The first 2 rows are our models with GNMT
attention:
<a href="http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_4_layer.zip" rel="nofollow">model 1 (4 layers)</a>,
<a href="http://download.tensorflow.org/models/nmt/10122017/ende_gnmt_model_8_layer.zip" rel="nofollow">model 2 (8 layers)</a>.</p>
<table>
<thead>
<tr>
<th>Systems</th>
<th align="center">newstest2014</th>
<th align="center">newstest2015</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Ours</em> — NMT + GNMT attention (4 layers)</td>
<td align="center">23.7</td>
<td align="center">26.5</td>
</tr>
<tr>
<td><em>Ours</em> — NMT + GNMT attention (8 layers)</td>
<td align="center">24.4</td>
<td align="center"><strong>27.6</strong></td>
</tr>
<tr>
<td><a href="http://matrix.statmt.org/" rel="nofollow">WMT SOTA</a></td>
<td align="center">20.6</td>
<td align="center">24.9</td>
</tr>
<tr>
<td>OpenNMT <a href="https://arxiv.org/abs/1701.02810" rel="nofollow">(Klein et al., 2017)</a></td>
<td align="center">19.3</td>
<td align="center">-</td>
</tr>
<tr>
<td>tf-seq2seq <a href="https://arxiv.org/abs/1703.03906" rel="nofollow">(Britz et al., 2017)</a></td>
<td align="center">22.2</td>
<td align="center">25.2</td>
</tr>
<tr>
<td>GNMT <a href="https://research.google.com/pubs/pub45610.html" rel="nofollow">(Wu et al., 2016)</a></td>
<td align="center"><strong>24.6</strong></td>
<td align="center">-</td>
</tr></tbody></table>
<p>The above results show our models are very competitive among models of similar architectures.<br>
[Note that OpenNMT uses smaller models and the current best result (as 
of this writing) is 28.4 obtained by the Transformer network <a href="https://arxiv.org/abs/1706.03762" rel="nofollow">(Vaswani et al., 2017)</a> which has a significantly different architecture.]</p>
<h2><a id="user-content-standard-hparams" class="anchor" aria-hidden="true" href="#standard-hparams"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Standard HParams</h2>
<p>We have provided
<a href="https://github.com/tensorflow/nmt/blob/master/nmt/standard_hparams">a set of standard hparams</a>
for using pre-trained checkpoint for inference or training NMT architectures
used in the Benchmark.</p>
<p>We will use the WMT16 German-English data, you can download the data by the
following command.</p>
<pre><code>nmt/scripts/wmt16_en_de.sh /tmp/wmt16
</code></pre>
<p>Here is an example command for loading the pre-trained GNMT WMT German-English
checkpoint for inference.</p>
<pre><code>python -m nmt.nmt \
    --src=de --tgt=en \
    --ckpt=/path/to/checkpoint/translate.ckpt \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \
    --inference_output_file=/tmp/deen_gnmt/output_infer \
    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en
</code></pre>
<p>Here is an example command for training the GNMT WMT German-English model.</p>
<pre><code>python -m nmt.nmt \
    --src=de --tgt=en \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \
    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \
    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000
</code></pre>
<h1><a id="user-content-other-resources" class="anchor" aria-hidden="true" href="#other-resources"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Other resources</h1>
<p>For deeper reading on Neural Machine Translation and sequence-to-sequence
models, we highly recommend the following materials
by
<a href="https://sites.google.com/site/acl16nmt/" rel="nofollow">Luong, Cho, Manning, (2016)</a>;
<a href="https://github.com/lmthang/thesis">Luong, (2016)</a>;
and <a href="https://arxiv.org/abs/1703.01619" rel="nofollow">Neubig, (2017)</a>.</p>
<p>There's a wide variety of tools for building seq2seq models, so we pick one per
language:<br>
Stanford NMT
<a href="https://nlp.stanford.edu/projects/nmt/" rel="nofollow">https://nlp.stanford.edu/projects/nmt/</a>
<em>[Matlab]</em> <br>
tf-seq2seq
<a href="https://github.com/google/seq2seq">https://github.com/google/seq2seq</a>
<em>[TensorFlow]</em> <br>
Nemantus
<a href="https://github.com/rsennrich/nematus">https://github.com/rsennrich/nematus</a>
<em>[Theano]</em> <br>
OpenNMT <a href="http://opennmt.net/" rel="nofollow">http://opennmt.net/</a> <em>[Torch]</em><br>
OpenNMT-py <a href="https://github.com/OpenNMT/OpenNMT-py">https://github.com/OpenNMT/OpenNMT-py</a> <em>[PyTorch]</em></p>
<h1><a id="user-content-acknowledgment" class="anchor" aria-hidden="true" href="#acknowledgment"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Acknowledgment</h1>
<p>We would like to thank Denny Britz, Anna Goldie, Derek Murray, and 
Cinjon Resnick for their work bringing new features to TensorFlow and 
the seq2seq library. Additional thanks go to Lukasz Kaiser for the 
initial help on the seq2seq codebase; Quoc Le for the suggestion to 
replicate GNMT; Yonghui Wu and Zhifeng Chen for details on the GNMT 
systems; as well as the Google Brain team for their support and 
feedback!</p>
<h1><a id="user-content-references" class="anchor" aria-hidden="true" href="#references"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h1>
<ul>
<li>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2015.<a href="https://arxiv.org/pdf/1409.0473.pdf" rel="nofollow"> Neural machine translation by jointly learning to align and translate</a>. ICLR.</li>
<li>Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015.<a href="https://arxiv.org/pdf/1508.04025.pdf" rel="nofollow"> Effective approaches to attention-based neural machine translation</a>. EMNLP.</li>
<li>Ilya Sutskever, Oriol Vinyals, and Quoc
V. Le. 2014.<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow"> Sequence to sequence learning with neural networks</a>. NIPS.</li>
</ul>
<h1><a id="user-content-bibtex" class="anchor" aria-hidden="true" href="#bibtex"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>BibTex</h1>
<pre><code>@article{luong17,
  author  = {Minh{-}Thang Luong and Eugene Brevdo and Rui Zhao},
  title   = {Neural Machine Translation (seq2seq) Tutorial},
  journal = {https://github.com/tensorflow/nmt},
  year    = {2017},
}
</code></pre>
</article>
      </div>
  </div>


  </div>
  <div class="modal-backdrop js-touch-events"></div>
</div>

    </div>
  </div>

  </div>

        
<div class="footer container-lg px-3" role="contentinfo">
  <div class="position-relative d-flex flex-justify-between pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <ul class="list-style-none d-flex flex-wrap ">
      <li class="mr-3">© 2018 <span title="0.24147s from unicorn-7967df4c7c-249v5">GitHub</span>, Inc.</li>
        <li class="mr-3"><a data-ga-click="Footer, go to terms, text:terms" href="https://github.com/site/terms">Terms</a></li>
        <li class="mr-3"><a data-ga-click="Footer, go to privacy, text:privacy" href="https://github.com/site/privacy">Privacy</a></li>
        <li class="mr-3"><a href="https://help.github.com/articles/github-security/" data-ga-click="Footer, go to security, text:security">Security</a></li>
        <li class="mr-3"><a href="https://status.github.com/" data-ga-click="Footer, go to status, text:status">Status</a></li>
        <li><a data-ga-click="Footer, go to help, text:help" href="https://help.github.com/">Help</a></li>
    </ul>

    <a aria-label="Homepage" title="GitHub" class="footer-octicon" href="https://github.com/">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24" aria-hidden="true"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg>
</a>
   <ul class="list-style-none d-flex flex-wrap ">
        <li class="mr-3"><a data-ga-click="Footer, go to contact, text:contact" href="https://github.com/contact">Contact GitHub</a></li>
        <li class="mr-3"><a href="https://github.com/pricing" data-ga-click="Footer, go to Pricing, text:Pricing">Pricing</a></li>
      <li class="mr-3"><a href="https://developer.github.com/" data-ga-click="Footer, go to api, text:api">API</a></li>
      <li class="mr-3"><a href="https://training.github.com/" data-ga-click="Footer, go to training, text:training">Training</a></li>
        <li class="mr-3"><a href="https://blog.github.com/" data-ga-click="Footer, go to blog, text:blog">Blog</a></li>
        <li><a data-ga-click="Footer, go to about, text:about" href="https://github.com/about">About</a></li>

    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>
</div>



  <div id="ajax-error-message" class="ajax-error-message flash flash-error">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg>
    <button type="button" class="flash-close js-ajax-error-dismiss" aria-label="Dismiss error">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"></path></svg>
    </button>
    You can’t perform that action at this time.
  </div>


    <script crossorigin="anonymous" integrity="sha512-2VdGgXQE8W5ONZ4OsrbEo/noennUZaqXkevD9R8juTHiCsjT0HFTTF6MoBfySUc8G+eFqUcDgd7v+CAu8Gjxlg==" type="application/javascript" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/compat-f849c975b0ffaa01d6ca305e48417d08.js"></script>
    <script crossorigin="anonymous" integrity="sha512-5Y3LREAfvm+nZ6YjoZt8WVf7lw7GnaeMghM+ILrhL7L4T9o2ZBSPibGcpU7os/YCfi7xtcB9cZZErGP9G2vJ2g==" type="application/javascript" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/frameworks-320b28cdf5601867c9f1610023761057.js"></script>
    
    <script crossorigin="anonymous" async="async" integrity="sha512-CWuGE6uT6lPOk1YqJcDZaA0Z0COX8Wqpnu7S0i17RP3AuTWkuK6CnOzGEr1W8PxgKyVHXOnaIFjotirsw2xuoA==" type="application/javascript" src="tensorflow_nmt%20%20TensorFlow%20Neural%20Machine%20Translation%20Tutorial_files/github-caee75d507e9009721778241c2aa2e63.js"></script>
    
    
    
  <div class="js-stale-session-flash stale-session-flash flash flash-warn flash-banner d-none">
    <svg class="octicon octicon-alert" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg>
    <span class="signed-in-tab-flash">You signed in with another tab or window. <a href="">Reload</a> to refresh your session.</span>
    <span class="signed-out-tab-flash">You signed out in another tab or window. <a href="">Reload</a> to refresh your session.</span>
  </div>
  <div class="facebox" id="facebox" style="display:none;">
  <div class="facebox-popup">
    <div class="facebox-content" role="dialog" aria-labelledby="facebox-header" aria-describedby="facebox-description">
    </div>
    <button type="button" class="facebox-close js-facebox-close" aria-label="Close modal">
      <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"></path></svg>
    </button>
  </div>
</div>

  <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default text-gray-dark" open="">
    <summary aria-haspopup="dialog" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast">
      <button class="m-3 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg class="octicon octicon-x" viewBox="0 0 12 16" version="1.1" width="12" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M7.48 8l3.75 3.75-1.48 1.48L6 9.48l-3.75 3.75-1.48-1.48L4.52 8 .77 4.25l1.48-1.48L6 6.52l3.75-3.75 1.48 1.48L7.48 8z"></path></svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

  <div class="Popover js-hovercard-content position-absolute" style="display: none; outline: none;" tabindex="0">
  <div class="Popover-message Popover-message--bottom-left Popover-message--large Box box-shadow-large" style="width:360px;">
  </div>
</div>

<div id="hovercard-aria-description" class="sr-only">
  Press h to open a hovercard with more details.
</div>


  


</body></html>